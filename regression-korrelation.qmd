---
title: Regression und Korrelation
---

Im vorherigen Kapitel haben wir gelernt, wie man die Verteilung einer einzelnen Variable in einer Stichprobe beschreibt. Allerdings müssen in den  meisten Fällen mehrere Variablen beschrieben werden, die oft miteinander verbunden sind. Beispielsweise sollte eine Ernährungsstudie alle Variablen berücksichtigen, die mit dem Gewicht in Zusammenhang stehen könnten, wie z.B. Größe, Alter, Geschlecht, Rauchen, Ernährung und körperliche  Betätigung.


Um ein Phänomen zu verstehen, das mehrere Variablen beinhaltet, reicht es nicht aus, jede Variable für sich allein zu studieren. Wir müssen alle Variablen gemeinsam untersuchen, um die Art ihrer Wechselbeziehungen und den Typ der Beziehung zwischen ihnen zu beschreiben.

In der Regel gibt es bei einer Abhängigkeitsstudie eine **abhängige** Variable $Y$, die von einem Satz an **unabhängigen** Variablen $X_1 , \dots , X_n$ beeinflusst wird. Der einfachste Fall ist eine einfache Abhängigkeitsstudie mit nur einer unabhängigen Variable. 



## Gemeinsame Häufigkeiten

Um die Beziehung zwischen zwei Variablen $X$ und $Y$ zu untersuchen, müssen wir die gemeinsame Verteilung der zweidimensionalen Variable $(X, Y)$ studieren, deren Werte Paare $(x_i, y_j)$ sind, wobei das erste Element ein Wert von $X$ und das zweite ein Wert von $Y$ ist.

::: {.callout-warning appearance="default"}
## Definition "gemeinsame Häufigkeiten"
Bei einer Stichprobe mit $n$ Werten und einer zweidimensionalen Variablen $(X, Y)$, wird für jeden  Wert der Variablen $(x_i, y_j)$ folgendes definiert:

- Absolute Häufigkeit $n_{ij}$: Ist die Anzahl der Male, die das Paar $(x_i, y_j)$ in der Stichprobe vorkommt.
- Relative Häufigkeit $n_{ij}$: Ist der Anteil der Male, die das Paar $(x_i, y_j)$ in der Stichprobe vorkommt.
$$
f_{ij}=\frac{n_{ij}}{n}
$$
:::

::: {.callout-important appearance="default"}
## Achtung! 
Für zweidimensionale Variablen ergeben kumulative Häufigkeiten keinen Sinn.
:::





### gemeinsame Häufigkeitsverteilung

Die Werte der zweidimensionalen Variablen mit ihren Häufigkeiten werden als gemeinsame Häufigkeitsverteilung bezeichnet und in einer gemeinsamen Häufigkeitstabelle dargestellt.

$$
\begin{array}{|c|ccccc|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_i & n_{i1} & \cdots & n_{ij} & \cdots & n_{iq}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq}\\
\hline
\end{array}
$$

::: {.callout-note appearance="default"}
## Beispiel

Von 30 Studierenden wurden Körpergröße (in cm) und Gewicht (in kg) wie folgt gemessen:

> (179,85), (173,65), (181,71), (170,65), (158,51), (174,66), (172,62), (166,60), (194,90), (185,75), (162,55), (187,78), (198,109), (177,61), (178,70), (165,58), (154,50), (183,93), (166,51), (171,65), (175,70), (182,60), (167,59), (169,62), (172,70), (186,71), (172,54), (176,68),(168,67), (187,80).

:::



Die gemeinsame Häufigkeitstabelle ist entsprechend:

$$
\begin{array}{|c||c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) \\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 \\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 \\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 \\
  \hline
\end{array}
$$

### Streudiagramm

Die gemeinsame Häufigkeitsverteilung kann mit einem Streudiagramm (Punktwolke, Scatterplot) graphisch dargestellt werden, wobei die Daten als Sammlung von Punkten in einem $XY$-Koordinatensystem angezeigt werden.

Üblicherweise wird die unabhängige Variable auf der $X$-Achse und die abhängige Variable auf der $Y$-Achse dargestellt. Für jedes Datenpaar $(x_i, y_j)$ in der Stichprobe wird ein Punkt mit diesen Koordinaten auf der Ebene gezeichnet.


```{r}
#| label: scatter1

df <- data.frame(x=2, y=2)

plot(df, 
     main=" ",
     xlab="Variable X",
     ylab="Variable Y",
     xaxt="n",
     yaxt="n",
     xlim=c(1,3),
     ylim=c(1,3),
     col="blue", lwd=3
     )
lines(c(0, 2), c(2, 2), lty=3)
lines(c(2, 2), c(0, 2), lty=3)
text(2.15,2.1, expression("(" * x[i] * "," * y[j] * ")"), cex=1.5, col="blue")
axis(side = 1, at = c(1:3), c(labels = expression(x[1]), 
                              labels = expression(x[i]),
                              labels = expression(x[n])), cex.axis = 1.2)
axis(side = 2, at = c(1:3), c(labels = expression(y[1]), 
                              labels = expression(y[j]),
                              labels = expression(y[n])), cex.axis = 1.2)
```

Das Ergebnis ist eine Ansammlung von Punkten, die auch *Punktwolke* genannt wird.


```{r}
#| label: regression1

data <- read.table("data/height_weight_data.csv", 
                   header=TRUE, 
                   sep="\t", 
                   na.strings="NA", 
                   dec=".", 
                   strip.white=TRUE)
# Height and weight scatter plot
par(mar=c(2.9,2.9,2,1), 
    mgp=c(1.8,0.4,0), 
    cex.lab=1, 
    cex.axis=0.8, 
    las=1, 
    tck=-0.02)
plot(weight~height, 
     col="skyblue", 
     main="Punktwolke von Größe und Gewicht", 
     xlab="Größe (cm)", 
     ylab="Gewicht (kg)", 
     xlim=c(150,200), 
     pch=16, 
     data=data)
segments(179,0,179,85, lty=2, col="grey")
segments(0,85,179,85, lty=2, col="grey")
text(179,87,"(179,85)")

```

Die Form der Punktwolke gibt Aufschluss über die Art der Beziehung zwischen den Variablen $X$ und $Y$.


```{r}
#| label: regression2
par(mfrow=c(2,3))
par(mar=c(1,1,1.4,1))
x=runif(100,-10,10)
y=runif(100,-10,10)
plot(x,y, col="steelblue", main="No relation", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()

# Linear relation scatter plot
#tikz(file="img/regression/linear_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=0.5*x+rnorm(100,0,1)
plot(x,y, col="steelblue", main="Lineare Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Quadratic relation scatter plot
#tikz(file="img/regression/quadratic_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=(x+rnorm(100,0,1))^2
plot(x,y, col="steelblue", main="Quadratische Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Exponential relation scatter plot
#tikz(file="img/regression/exponential_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=exp(0.3*x+rnorm(100,0,0.1))+rnorm(100,0,1)
plot(x,y, col="steelblue", main="Exponentielle Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Logarithmic relation scatter plot
#tikz(file="img/regression/logarithmic_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
x=runif(100,0,10)
y=log(x+rnorm(100,1,0.3),2)+rnorm(100,0,0.1)
plot(x,y, col="steelblue", main="Logarithmische Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Inverse relation scatter plot
#tikz(file="img/regression/inverse_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
x=runif(100,0.5,5)
y=1/(x+rnorm(100,0,0.1))+rnorm(100,0,0.1)
plot(x,y, col="steelblue", main="Inverse Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()

```





#### Häufigkeitsverteilungen an den Rändern

Die Häufigkeitsverteilungen jeder der zweidimensionalen Variablen werden als Randhäufigkeitsverteilungen bezeichnet.
Wir können die Randhäufigkeitsverteilungen aus der gemeinsamen Häufigkeitstabelle erhalten, indem wir die Häufigkeiten nach Zeilen und Spalten summieren.

$$
\begin{array}{|c|ccccc|>{\columncolor{blue}}c|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q & \cellcolor{skyblue}n_x\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q} & \cellcolor{skyblue}n_{x_1}\\
\vdots & \vdots & \vdots & \cellcolor{coral}+ \downarrow& \vdots & \vdots & \cellcolor{skyblue}\vdots \\
x_i & n_{i1} & \cellcolor{skyblue}+\rightarrow & n_{ij} &\cellcolor{skyblue} +\rightarrow & n_{iq} & \cellcolor{skyblue} n_{x_i}\\
\vdots & \vdots & \vdots & \cellcolor{coral}+\downarrow & \vdots & \vdots & \cellcolor{skyblue}\vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq} & \cellcolor{skyblue}n_{x_p} \\
\hline
\rowcolor{coral}
n_y & n_{y_1} & \cdots & n_{y_j} & \cdots & n_{y_q} & \cellcolor{white} n\\
\hline
\end{array}
$$




Die Randhäufigkeitsverteilungen von Körpergröße und Gewicht sind

$$
\begin{array}{|c||c|c|c|c|c|c|>{\columncolor{blue}}c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) &\cellcolor{skyblue} n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & \cellcolor{skyblue}2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & \cellcolor{skyblue}8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & \cellcolor{skyblue}11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & \cellcolor{skyblue}7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & \cellcolor{skyblue}2\\
  \hline
  \rowcolor{coral}
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & \cellcolor{white} 30\\
  \hline
\end{array}
$$

mit den den dazugehörigen Stichprobenstatistiken:

$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2 & \quad & s_x = 10.1 \mbox{ cm}\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2 & & s_y = 12.82 \mbox{ Kg}
\end{array}
$$





## Kovarianz
### Abweichungen von den Mittelwerten
Um die Beziehung zwischen zwei Variablen zu untersuchen, müssen wir ihre gemeinsame Variation analysieren.

```{r}
#| label: kovarianz1
library(shape)
par(mar=c(2.9, 2.9, 0.2, 1), 
    mgp=c(1.8, 0.4, 0), 
    cex.lab=1, 
    cex.axis=1, 
    las=1, 
    tck=-0.02)

plot(weight~height, 
     col="steelblue", 
     main="", 
     xlab=expression(X), 
     ylab=expression(Y), 
     xlim=c(150,200), 
     pch=16, 
     data=data, 
     axes=F)
box()
axis(1, at = c(174.67), labels = expression(bar(x)))
axis(2, at = c(69.67),  labels = expression(bar(y)), las = 2)

# Hilfslinien
abline(h = 69.67,  lty = 2, col = "grey")
abline(v = 174.67, lty = 2, col = "grey")

# Punkt (Mittelwert)
points(174.67, 69.67, pch = 16, col = "darkred")

# Beschriftungen
text(172, 73, expression("(" * bar(x) * "," * bar(y) * ")"), col = "darkred")
text(197, 93, expression("(" * x[i] * "," * y[j] * ")"))

# Horizontaler Pfeil: x_i - x̄
Arrows(174.67, 90, 194, 90, 
       arr.length = 0.2, 
       arr.width = 0.15, 
       code = 3, 
       arr.type = "triangle", 
       arr.adj = 1, 
       col = "darkred")
text(186, 92, 
     expression(x[i] - bar(x)), 
     col = "darkred")

# Vertikaler Pfeil: y_j - ȳ
Arrows(194, 69.67, 194, 90, 
       arr.length = 0.2, 
       arr.width = 0.15, 
       code = 3, 
       arr.type = "triangle", 
       arr.adj = 1, 
       col = "darkred")
text(198, 80, 
     expression(y[j] - bar(y)), 
     col = "darkred")

```

Wenn wir das Diagramm der Punktwolke in 4 Quadranten mit dem Mittelpunkt
$(\bar x, \bar y)$ unterteilen, so sind die Vorzeichen der Abweichungen vom Mittelwert:


$$
\begin{array}{cccc}
Quadrant & (x_i-\bar x) & (y_j-\bar y) & (x_i-\bar x)(y_j-\bar y)\\
\hline
1 & + & + & \mathbf{+}\\
2 & - & + & \mathbf{-}\\
3 & - & - & \mathbf{+}\\
4 & + & - & \mathbf{-}\\
\hline
\end{array}
$$



```{r}
#| label: quadranten1
x=2
y=2
plot(x,y, 
     col="steelblue",
     xlim=c(0,4),
     ylim=c(0,4),
     main=" ", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=2, lty=2,col="grey")
abline(v=2, lty=2,col="grey")
text(x=0.1, y=3.9, labels="1")
text(x=3.9, y=3.9, labels="2")
text(x=0.1, y=0.1, labels="3")
text(x=3.9, y=0.1, labels="4")
text(x=1, y=1, labels="+", cex=3, col="darkred")
text(x=3, y=1, labels="-", cex=3, col="darkred")
text(x=3, y=3, labels="+", cex=3, col="darkred")
text(x=1, y=3, labels="-", cex=3, col="darkred")
```

::: {.callout-note appearance="minimal"}
Wenn zwischen den Variablen eine *aufsteigende* lineare Beziehung besteht, werden die meisten Punkte in den Quadranten 1 und 3 liegen und die Summe der Produkte der Abweichungen vom Mittelwert wird *positiv* sein.

```{r}
#| label: quadranten2
x=runif(100)
y=0.5*x+rnorm(100,0,0.05)
mx=mean(x)
my=mean(y)
par(mai=c(0.3,0.3,0.4,0.2), mgp=c(0.5,0,0))
plot(x,y, 
     col="steelblue", 
     main="steigender linearer Zusammenhang", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=my, lty=2,col="grey")
abline(v=mx, lty=2,col="grey")

```

$$
\sum(x_i-\bar x)(y_j-\bar y) >0
$$
:::





::: {.callout-note appearance="minimal"}

Wenn zwischen den Variablen eine *abnehmende* lineare Beziehung besteht, werden die meisten Punkte in den Quadranten 2 und 4 liegen und die Summe der Produkte der Abweichungen vom Mittelwert wird *negativ* sein.


```{r}
#| label: quadranten3
x=runif(100)
y=-0.5*x+rnorm(100, 0, 0.05)
mx=mean(x)
my=mean(y)
par(mai=c(0.3, 0.3, 0.4, 0.2), mgp=c(0.5, 0, 0))
plot(x,y, 
     col="steelblue", 
     main="abnehmender linearer Zusammenhang", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=my, lty=2,col="grey")
abline(v=mx, lty=2,col="grey")


```


$$
\sum(x_i-\bar x)(y_j-\bar y) <0
$$
:::


### Kovarianz

::: {.callout-warning appearance="default"}
## Definition "Kovarianz"
Die Kovarianz einer zweidimensionalen Variablen $(X, Y)$ ist der Durchschnitt der Produkte der 
Abweichungen von den jeweiligen Mitteln.

$$
s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}
$$
:::


Sie kann auch unter Verwendung der folgenden Formel berechnet werden:

$$
s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y
$$

Die Kovarianz misst die lineare Beziehung zwischen zwei Variablen:

- Wenn $s_{xy}$ > 0, besteht eine zunehmende lineare Beziehung.
- Wenn $s_{xy}$ < 0, besteht eine abnehmende lineare Beziehung.
- Wenn $s_{xy}$ = 0, gibt es keine lineare Beziehung.

Verwenden wir die Randhäufigkeitsverteilungen von Körpergröße und Gewicht 

$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\
  \hline
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & 30\\
  \hline
\end{array}
$$

...mit den Mittelwerten.
$$
\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}
$$

So lässt sich die Kovarianz wie folgt berechnen:

$$
\begin{aligned}
s_{xy} &=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y \\
& =  \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 =\\
& = \frac{368200}{30}-12169.26 = 104.07 \text{cm} \cdot \text{Kg}
\end{aligned}
$$

Das bedeutet, dass eine positive lineare Beziehung zwischen den Variablen besteht.



## Regression

In den meisten Fällen ist das Ziel einer Abhängigkeitsstudie nicht nur die Detektion einer Beziehung zwischen zwei Variablen, sondern auch die  Expression dieser Beziehung mit einer mathematischen Funktion,
$$
y=f(x)
$$

um die abhängige Variable $y$ für jeden Wert der unabhängigen $x$ vorherzusagen.

Der Teil der Statistik, der sich mit dem Aufbau solch einer Funktion beschäftigt, wird Regression genannt, und die Funktion selbst heißt  *Regressionsfunktion* oder *Regressionsmodell*.


### Einfache Regressionsmodelle

Es gibt viele Arten von Regressionsmodellen. Die häufigsten Modelle sind in der folgenden Tabelle aufgeführt.

$$
\begin{array}{ll}
\textbf{Model} & \textbf{Gleichung}\\
Linear & y=a+bx\\
Quadratisch & y=a+bx+cx^2\\
Kubisch & y=a+bx+cx^2+dx^3\\
Potenz & y=a\cdot x^b\\
Exponentiell & y=e^{a+bx}\\
Logarithmisch & y=a+b\cdot\log(x)\\
Invers & y = a+\frac{b}{x}\\
Sigmoidal & y= e^{a+\frac{b}{x}}\\
\end{array}
$$

