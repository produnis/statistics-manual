---
title: Regression und Korrelation
---

Im vorherigen Kapitel haben wir gelernt, wie man die Verteilung einer einzelnen Variable in einer Stichprobe beschreibt. Allerdings müssen in den  meisten Fällen mehrere Variablen beschrieben werden, die oft miteinander verbunden sind. Beispielsweise sollte eine Ernährungsstudie alle Variablen berücksichtigen, die mit dem Gewicht in Zusammenhang stehen könnten, wie z.B. Größe, Alter, Geschlecht, Rauchen, Ernährung und körperliche  Betätigung.


Um ein Phänomen zu verstehen, das mehrere Variablen beinhaltet, reicht es nicht aus, jede Variable für sich allein zu studieren. Wir müssen alle Variablen gemeinsam untersuchen, um die Art ihrer Wechselbeziehungen und den Typ der Beziehung zwischen ihnen zu beschreiben.

In der Regel gibt es bei einer Abhängigkeitsstudie eine **abhängige** Variable $Y$, die von einem Satz an **unabhängigen** Variablen $X_1 , \dots , X_n$ beeinflusst wird. Der einfachste Fall ist eine einfache Abhängigkeitsstudie mit nur einer unabhängigen Variable. 



## Gemeinsame Häufigkeiten

Um die Beziehung zwischen zwei Variablen $X$ und $Y$ zu untersuchen, müssen wir die gemeinsame Verteilung der zweidimensionalen Variable $(X, Y)$ studieren, deren Werte Paare $(x_i, y_j)$ sind, wobei das erste Element ein Wert von $X$ und das zweite ein Wert von $Y$ ist.

::: {.callout-warning appearance="default"}
## Definition "gemeinsame Häufigkeiten"
Bei einer Stichprobe mit $n$ Werten und einer zweidimensionalen Variablen $(X, Y)$, wird für jeden  Wert der Variablen $(x_i, y_j)$ folgendes definiert:

- Absolute Häufigkeit $n_{ij}$: Ist die Anzahl der Male, die das Paar $(x_i, y_j)$ in der Stichprobe vorkommt.
- Relative Häufigkeit $n_{ij}$: Ist der Anteil der Male, die das Paar $(x_i, y_j)$ in der Stichprobe vorkommt.
$$
f_{ij}=\frac{n_{ij}}{n}
$$
:::

::: {.callout-important appearance="default"}
## Achtung! 
Für zweidimensionale Variablen ergeben kumulative Häufigkeiten keinen Sinn.
:::





### gemeinsame Häufigkeitsverteilung

Die Werte der zweidimensionalen Variablen mit ihren Häufigkeiten werden als gemeinsame Häufigkeitsverteilung bezeichnet und in einer gemeinsamen Häufigkeitstabelle dargestellt.

$$
\begin{array}{|c|ccccc|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_i & n_{i1} & \cdots & n_{ij} & \cdots & n_{iq}\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq}\\
\hline
\end{array}
$$

::: {.callout-note appearance="default"}
## Beispiel

Von 30 Studierenden wurden Körpergröße (in cm) und Gewicht (in kg) wie folgt gemessen:

> (179,85), (173,65), (181,71), (170,65), (158,51), (174,66), (172,62), (166,60), (194,90), (185,75), (162,55), (187,78), (198,109), (177,61), (178,70), (165,58), (154,50), (183,93), (166,51), (171,65), (175,70), (182,60), (167,59), (169,62), (172,70), (186,71), (172,54), (176,68),(168,67), (187,80).

:::



Die gemeinsame Häufigkeitstabelle ist entsprechend:

$$
\begin{array}{|c||c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) \\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 \\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 \\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 \\
  \hline
\end{array}
$$

### Streudiagramm

Die gemeinsame Häufigkeitsverteilung kann mit einem Streudiagramm (Punktwolke, Scatterplot) graphisch dargestellt werden, wobei die Daten als Sammlung von Punkten in einem $XY$-Koordinatensystem angezeigt werden.

Üblicherweise wird die unabhängige Variable auf der $X$-Achse und die abhängige Variable auf der $Y$-Achse dargestellt. Für jedes Datenpaar $(x_i, y_j)$ in der Stichprobe wird ein Punkt mit diesen Koordinaten auf der Ebene gezeichnet.


```{r}
#| label: scatter1

df <- data.frame(x=2, y=2)

plot(df, 
     main=" ",
     xlab="Variable X",
     ylab="Variable Y",
     xaxt="n",
     yaxt="n",
     xlim=c(1,3),
     ylim=c(1,3),
     col="blue", lwd=3
     )
lines(c(0, 2), c(2, 2), lty=3)
lines(c(2, 2), c(0, 2), lty=3)
text(2.15,2.1, expression("(" * x[i] * "," * y[j] * ")"), cex=1.5, col="blue")
axis(side = 1, at = c(1:3), c(labels = expression(x[1]), 
                              labels = expression(x[i]),
                              labels = expression(x[n])), cex.axis = 1.2)
axis(side = 2, at = c(1:3), c(labels = expression(y[1]), 
                              labels = expression(y[j]),
                              labels = expression(y[n])), cex.axis = 1.2)
```

Das Ergebnis ist eine Ansammlung von Punkten, die auch *Punktwolke* genannt wird.


```{r}
#| label: regression1

data <- read.table("data/height_weight_data.csv", 
                   header=TRUE, 
                   sep="\t", 
                   na.strings="NA", 
                   dec=".", 
                   strip.white=TRUE)
# Height and weight scatter plot
par(mar=c(2.9,2.9,2,1), 
    mgp=c(1.8,0.4,0), 
    cex.lab=1, 
    cex.axis=0.8, 
    las=1, 
    tck=-0.02)
plot(weight~height, 
     col="skyblue", 
     main="Punktwolke von Größe und Gewicht", 
     xlab="Größe (cm)", 
     ylab="Gewicht (kg)", 
     xlim=c(150,200), 
     pch=16, 
     data=data)
segments(179,0,179,85, lty=2, col="grey")
segments(0,85,179,85, lty=2, col="grey")
text(179,87,"(179,85)")

```

Die Form der Punktwolke gibt Aufschluss über die Art der Beziehung zwischen den Variablen $X$ und $Y$.


```{r}
#| label: regression2
par(mfrow=c(2,3))
par(mar=c(1,1,1.4,1))
x=runif(100,-10,10)
y=runif(100,-10,10)
plot(x,y, col="steelblue", main="No relation", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()

# Linear relation scatter plot
#tikz(file="img/regression/linear_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=0.5*x+rnorm(100,0,1)
plot(x,y, col="steelblue", main="Lineare Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Quadratic relation scatter plot
#tikz(file="img/regression/quadratic_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=(x+rnorm(100,0,1))^2
plot(x,y, col="steelblue", main="Quadratische Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Exponential relation scatter plot
#tikz(file="img/regression/exponential_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
y=exp(0.3*x+rnorm(100,0,0.1))+rnorm(100,0,1)
plot(x,y, col="steelblue", main="Exponentielle Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Logarithmic relation scatter plot
#tikz(file="img/regression/logarithmic_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
x=runif(100,0,10)
y=log(x+rnorm(100,1,0.3),2)+rnorm(100,0,0.1)
plot(x,y, col="steelblue", main="Logarithmische Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()
#dev.off()

# Inverse relation scatter plot
#tikz(file="img/regression/inverse_scatterplot.tex", width=7, height=5)
par(mar=c(1,1,1.4,1))
x=runif(100,0.5,5)
y=1/(x+rnorm(100,0,0.1))+rnorm(100,0,0.1)
plot(x,y, col="steelblue", main="Inverse Beziehung", xlab="$X$", ylab="$Y$", pch=16, axes=F)
box()

```





#### Häufigkeitsverteilungen an den Rändern

Die Häufigkeitsverteilungen jeder der zweidimensionalen Variablen werden als Randhäufigkeitsverteilungen bezeichnet.
Wir können die Randhäufigkeitsverteilungen aus der gemeinsamen Häufigkeitstabelle erhalten, indem wir die Häufigkeiten nach Zeilen und Spalten summieren.

$$
\begin{array}{|c|ccccc|>{\columncolor{blue}}c|}
\hline
X\backslash Y & y_1 & \cdots & y_j & \cdots & y_q & \cellcolor{skyblue}n_x\\
\hline
x_1 & n_{11} & \cdots & n_{1j} & \cdots & n_{1q} & \cellcolor{skyblue}n_{x_1}\\
\vdots & \vdots & \vdots & \cellcolor{coral}+ \downarrow& \vdots & \vdots & \cellcolor{skyblue}\vdots \\
x_i & n_{i1} & \cellcolor{skyblue}+\rightarrow & n_{ij} &\cellcolor{skyblue} +\rightarrow & n_{iq} & \cellcolor{skyblue} n_{x_i}\\
\vdots & \vdots & \vdots & \cellcolor{coral}+\downarrow & \vdots & \vdots & \cellcolor{skyblue}\vdots\\
x_p & n_{p1} & \cdots & n_{pj} & \cdots & n_{pq} & \cellcolor{skyblue}n_{x_p} \\
\hline
\rowcolor{coral}
n_y & n_{y_1} & \cdots & n_{y_j} & \cdots & n_{y_q} & \cellcolor{white} n\\
\hline
\end{array}
$$




Die Randhäufigkeitsverteilungen von Körpergröße und Gewicht sind

$$
\begin{array}{|c||c|c|c|c|c|c|>{\columncolor{blue}}c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) &\cellcolor{skyblue} n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & \cellcolor{skyblue}2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & \cellcolor{skyblue}8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & \cellcolor{skyblue}11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & \cellcolor{skyblue}7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & \cellcolor{skyblue}2\\
  \hline
  \rowcolor{coral}
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & \cellcolor{white} 30\\
  \hline
\end{array}
$$

mit den den dazugehörigen Stichprobenstatistiken:

$$
\begin{array}{lllll}
\bar x = 174.67 \mbox{ cm} & \quad & s^2_x = 102.06 \mbox{ cm}^2 & \quad & s_x = 10.1 \mbox{ cm}\\
\bar y = 69.67 \mbox{ Kg} & & s^2_y = 164.42 \mbox{ Kg}^2 & & s_y = 12.82 \mbox{ Kg}
\end{array}
$$





## Kovarianz
### Abweichungen von den Mittelwerten
Um die Beziehung zwischen zwei Variablen zu untersuchen, müssen wir ihre gemeinsame Variation analysieren.

```{r}
#| label: kovarianz1
library(shape)
par(mar=c(2.9, 2.9, 0.2, 1), 
    mgp=c(1.8, 0.4, 0), 
    cex.lab=1, 
    cex.axis=1, 
    las=1, 
    tck=-0.02)

plot(weight~height, 
     col="steelblue", 
     main="", 
     xlab=expression(X), 
     ylab=expression(Y), 
     xlim=c(150,200), 
     pch=16, 
     data=data, 
     axes=F)
box()
axis(1, at = c(174.67), labels = expression(bar(x)))
axis(2, at = c(69.67),  labels = expression(bar(y)), las = 2)

# Hilfslinien
abline(h = 69.67,  lty = 2, col = "grey")
abline(v = 174.67, lty = 2, col = "grey")

# Punkt (Mittelwert)
points(174.67, 69.67, pch = 16, col = "darkred")

# Beschriftungen
text(172, 73, expression("(" * bar(x) * "," * bar(y) * ")"), col = "darkred")
text(197, 93, expression("(" * x[i] * "," * y[j] * ")"))

# Horizontaler Pfeil: x_i - x̄
Arrows(174.67, 90, 194, 90, 
       arr.length = 0.2, 
       arr.width = 0.15, 
       code = 3, 
       arr.type = "triangle", 
       arr.adj = 1, 
       col = "darkred")
text(186, 92, 
     expression(x[i] - bar(x)), 
     col = "darkred")

# Vertikaler Pfeil: y_j - ȳ
Arrows(194, 69.67, 194, 90, 
       arr.length = 0.2, 
       arr.width = 0.15, 
       code = 3, 
       arr.type = "triangle", 
       arr.adj = 1, 
       col = "darkred")
text(198, 80, 
     expression(y[j] - bar(y)), 
     col = "darkred")

```

Wenn wir das Diagramm der Punktwolke in 4 Quadranten mit dem Mittelpunkt
$(\bar x, \bar y)$ unterteilen, so sind die Vorzeichen der Abweichungen vom Mittelwert:


$$
\begin{array}{cccc}
Quadrant & (x_i-\bar x) & (y_j-\bar y) & (x_i-\bar x)(y_j-\bar y)\\
\hline
1 & + & + & \mathbf{+}\\
2 & - & + & \mathbf{-}\\
3 & - & - & \mathbf{+}\\
4 & + & - & \mathbf{-}\\
\hline
\end{array}
$$



```{r}
#| label: quadranten1
x=2
y=2
plot(x,y, 
     col="steelblue",
     xlim=c(0,4),
     ylim=c(0,4),
     main=" ", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=2, lty=2,col="grey")
abline(v=2, lty=2,col="grey")
text(x=0.1, y=3.9, labels="1")
text(x=3.9, y=3.9, labels="2")
text(x=0.1, y=0.1, labels="3")
text(x=3.9, y=0.1, labels="4")
text(x=1, y=1, labels="+", cex=3, col="darkred")
text(x=3, y=1, labels="-", cex=3, col="darkred")
text(x=3, y=3, labels="+", cex=3, col="darkred")
text(x=1, y=3, labels="-", cex=3, col="darkred")
```

::: {.callout-note appearance="minimal"}
Wenn zwischen den Variablen eine *aufsteigende* lineare Beziehung besteht, werden die meisten Punkte in den Quadranten 1 und 3 liegen und die Summe der Produkte der Abweichungen vom Mittelwert wird *positiv* sein.

```{r}
#| label: quadranten2
x=runif(100)
y=0.5*x+rnorm(100,0,0.05)
mx=mean(x)
my=mean(y)
par(mai=c(0.3,0.3,0.4,0.2), mgp=c(0.5,0,0))
plot(x,y, 
     col="steelblue", 
     main="steigender linearer Zusammenhang", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=my, lty=2,col="grey")
abline(v=mx, lty=2,col="grey")

```

$$
\sum(x_i-\bar x)(y_j-\bar y) >0
$$
:::





::: {.callout-note appearance="minimal"}

Wenn zwischen den Variablen eine *abnehmende* lineare Beziehung besteht, werden die meisten Punkte in den Quadranten 2 und 4 liegen und die Summe der Produkte der Abweichungen vom Mittelwert wird *negativ* sein.


```{r}
#| label: quadranten3
x=runif(100)
y=-0.5*x+rnorm(100, 0, 0.05)
mx=mean(x)
my=mean(y)
par(mai=c(0.3, 0.3, 0.4, 0.2), mgp=c(0.5, 0, 0))
plot(x,y, 
     col="steelblue", 
     main="abnehmender linearer Zusammenhang", 
     xlab=expression(X), 
     ylab=expression(Y), 
     pch=16, 
     axes=F)
box()
abline(h=my, lty=2,col="grey")
abline(v=mx, lty=2,col="grey")


```


$$
\sum(x_i-\bar x)(y_j-\bar y) <0
$$
:::


### Kovarianz

::: {.callout-warning appearance="default"}
## Definition "Kovarianz"
Die Kovarianz einer zweidimensionalen Variablen $(X, Y)$ ist der Durchschnitt der Produkte der 
Abweichungen von den jeweiligen Mitteln.

$$
s_{xy}=\frac{\sum (x_i-\bar x)(y_j-\bar y)n_{ij}}{n}
$$
:::


Sie kann auch unter Verwendung der folgenden Formel berechnet werden:

$$
s_{xy}=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y
$$

Die Kovarianz misst die lineare Beziehung zwischen zwei Variablen:

- Wenn $s_{xy}$ > 0, besteht eine zunehmende lineare Beziehung.
- Wenn $s_{xy}$ < 0, besteht eine abnehmende lineare Beziehung.
- Wenn $s_{xy}$ = 0, gibt es keine lineare Beziehung.

Verwenden wir die Randhäufigkeitsverteilungen von Körpergröße und Gewicht 

$$
\begin{array}{|c||c|c|c|c|c|c|c|}
\hline
  X/Y & [50,60) & [60,70) & [70,80) & [80,90) & [90,100) & [100,110) & n_x\\
  \hline\hline
  (150,160] & 2 & 0 & 0 & 0 & 0 & 0 & 2\\
  \hline
  (160,170] & 4 & 4 & 0 & 0 & 0 & 0 & 8\\
  \hline
  (170,180] & 1 & 6 & 3 & 1 & 0 & 0 & 11 \\
  \hline
  (180,190] & 0 & 1 & 4 & 1 & 1 & 0 & 7 \\
  \hline
  (190,200] & 0 & 0 & 0 & 0 & 1 & 1 & 2\\
  \hline
  n_y & 7 & 11 & 7 & 2 & 2 & 1 & 30\\
  \hline
\end{array}
$$

...mit den Mittelwerten.
$$
\bar x = 174.67 \mbox{ cm} \qquad \bar y = 69.67 \mbox{ Kg}
$$

So lässt sich die Kovarianz wie folgt berechnen:

$$
\begin{aligned}
s_{xy} &=\frac{\sum x_iy_jn_{ij}}{n}-\bar x\bar y \\
& =  \frac{155\cdot 55\cdot 2 + 165\cdot 55\cdot 4 + \cdots + 195\cdot 105\cdot 1}{30}-174.67\cdot 69.67 =\\
& = \frac{368200}{30}-12169.26 = 104.07 \text{cm} \cdot \text{Kg}
\end{aligned}
$$

Das bedeutet, dass eine positive lineare Beziehung zwischen den Variablen besteht.



## Regression

In den meisten Fällen ist das Ziel einer Abhängigkeitsstudie nicht nur die Detektion einer Beziehung zwischen zwei Variablen, sondern auch die  Expression dieser Beziehung mit einer mathematischen Funktion,
$$
y=f(x)
$$

um die abhängige Variable $y$ für jeden Wert der unabhängigen $x$ vorherzusagen.

Der Teil der Statistik, der sich mit dem Aufbau solch einer Funktion beschäftigt, wird Regression genannt, und die Funktion selbst heißt  *Regressionsfunktion* oder *Regressionsmodell*.


### Einfache Regressionsmodelle

Es gibt viele Arten von Regressionsmodellen. Die häufigsten Modelle sind in der folgenden Tabelle aufgeführt.

$$
\begin{array}{ll}
\textbf{Model} & \textbf{Gleichung}\\
Linear & y=a+bx\\
Quadratisch & y=a+bx+cx^2\\
Kubisch & y=a+bx+cx^2+dx^3\\
Potenz & y=a\cdot x^b\\
Exponentiell & y=e^{a+bx}\\
Logarithmisch & y=a+b\cdot\log(x)\\
Invers & y = a+\frac{b}{x}\\
Sigmoidal & y= e^{a+\frac{b}{x}}\\
\end{array}
$$

Die Wahl des Modells hängt von der Form der Punktwolke in der Streudiagramm ab.

### Vorhersagefehler

Sobald der Typ des Regressionsmodells gewählt wurde, müssen wir entscheiden, welche Funktion dieser Familie die Beziehung zwischen der abhängigen und  unabhängigen Variablen am besten erklärt, d.h. welche Funktion die abhängige Variable am besten vorhersagt.

Diese Funktion ist diejenige, die die Abstände von den beobachteten Werten für $Y$ in der Stichprobe zu den vorhergesagten Werten der Regressionsfunktion minimiert. Diese Abstände werden als *Restterme*, *Residuen* oder *Vorhersagefehler* bezeichnet.

::: {.callout-warning appearance="default"}
## Definition Vorhersagefehler: 

Gegeben ein Regressionsmodell $y = f(x)$ für eine zweidimensionale Variable $(X, Y)$, ist der  Vorhersagefehler für jedes Paar $(x_i, y_j)$ der Stichprobe die Differenz zwischen dem beobachteten Wert der abhängigen Variablen $y_j$ und dem  vorhergesagten Wert der Regressionsfunktion für $x_i$,

$$
e_{i,j} = y_j - f(x_i).
$$
:::



```{r}
#| label: residuen
par(mar=c(2.9,2.9,2,1), 
    mgp=c(1.8,0.4,0), 
    cex.lab=1, 
    cex.axis=1, 
    las=1, 
    tck=-0.02)
plot(weight~height, 
     col="steelblue", 
     main="", 
     xlab="X", 
     ylab="Y", 
     xlim=c(150,200), 
     pch=16, 
     data=data, 
     axes=F)
box()
model <- lm(data$weight~data$height)
abline(model, lwd=2)
prediction <- model[["coefficients"]][1]+model[["coefficients"]][2]*183
axis(1, at=c(183), labels=expression(x[i]))
axis(2, at = c(93, prediction), labels = expression(y[j], f(x[i])), las = 2)

# Pfeil von f(x_i) zu y_j
Arrows(183, prediction, 183, 93,
       arr.length = 0.2,
       arr.width = 0.15,
       code = 3,
       arr.type = "triangle",
       arr.adj = 1,
       col = "darkred")

# Hilfslinien
segments(183, 0, 183, prediction, col = "grey", lty = 2)
segments(0, prediction, 183, prediction, col = "grey", lty = 2)
segments(0, 93, 183, 93, col = "grey", lty = 2)

# Fehlerbezeichnung
text(179, 82, expression(e[ij] == y[j] - f(x[i])), col = "darkred")

# Punktbeschriftung
text(186, 95, expression("(" * x[i] * "," * y[j] * ")"))

```



#### Methode der kleinsten Quadrate

Um die Regressionsfunktion zu erhalten, kann die Methode der kleinsten Quadrate angewendet werde. Mit ihr wird die Funktion bestimmt, welche die quadrierten Residuen minimiert.

$$
\sum e_{ij}^2.
$$

Für ein lineares Modell $f(x) = a + bx$ hängt die Summe von zwei Parametern ab, dem Schnittpunkt durch die Y-Achse $a$ und der Steigung der Geraden $b$,

$$
\theta(a,b) = \sum e_{ij}^2 =\sum (y_j - f(x_i))^2 =\sum (y_j-a-bx_i)^2.
$$

Dies reduziert das Problem darauf, geeignete Werte für $a$ und $b$ zu bestimmen.


## Regressionsgerade

Um das Minimierungsproblem zu lösen, müssen wir die partiellen Ableitungen bezüglich $a$ und $b$ auf Null setzen.

$$
\begin{aligned}
\frac{\partial \theta(a,b)}{\partial a} &=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial a} =0\\
\frac{\partial \theta(a,b)}{\partial b} &=  \frac{\partial \sum (y_j-a-bx_i)^2 }{\partial b} =0
\end{aligned}
$$

Jetzt können wir das Gleichungssystem lösen zu:

$$
a= \bar y - \frac{s_{xy}}{s_x^2}\bar x \qquad b=\frac{s_{xy}}{s_x^2}
$$
Diese Formeln minimieren die Residuen von $Y$ und ergeben das optimale lineare Modell.



::: {.callout-warning appearance="default"}
## Definition "Regressiongerade" 

Für eine Stichprobe mit den zweidimensionalen Variablen $(X, Y)$ ist die Regressionsgerade von $Y$ auf $X$:

$$
y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x)
$$

:::

Die Regressionsgerade von $Y$ auf $X$ ist die Gerade, die die Vorhersagefehler von $Y$ minimiert und daher das lineare Regressionsmodell mit den besten Vorhersagen für $Y$ darstellt.

### Regressionlinienberechnung

Wir verwenden die Daten der Körpergrößen ($X$) und Körpergewichten ($Y$) mit den folgenden Kennwerten

$$
\begin{array}{lllll}
\bar x = 174.67 \text{ cm} & \quad & s^2_x = 102.06 \text{ cm}^2 & \quad & s_x = 10.1 \text{ cm}\\
\bar y = 69.67 \text{ Kg} & & s^2_y = 164.42 \text{ Kg}^2 & & s_y = 12.82 \text{ Kg}\\
& & s_{xy} = 104.07 \text{ cm}\cdot \text{ Kg} & &
\end{array}
$$

ergibt sich die Regressionsgerade für *Körpergröße erklärt durch Gewicht* (Größe gegen Gewicht, $Größe = f(Gewicht)$) wie folgt:

$$
y = \bar y +\frac{s_{xy}}{s_x^2}(x-\bar x) = 69.67+\frac{104.07}{102.06}(x-174.67) = -108.49 +1.02 x
$$

Die Regressionsgerade für *Gewicht erklärt durch Körpergröße* (Gewicht gegen Größe, $Gewicht = f(Größe)$) ist

$$
x = \bar x +\frac{s_{xy}}{s_y^2}(y-\bar y) = 174.67+\frac{104.07}{164.42}(y-69.67) = 130.78 + 0.63 y
$$

::: {.callout-important appearance="simple"}
Beachten Sie, dass die Regressionslinien unterschiedlich sind!
:::


```{r}
#| label: regressionslinien1
par(mar=c(2.9,2.9,2,1), 
    mgp=c(1.8,0.4,0), 
    cex.lab=1, 
    cex.axis=0.8, 
    las=1, 
    tck=-0.02)
plot(weight~height, 
     col="steelblue", 
     main="Regressionsgeraden von Größe und Gewicht", 
     xlab="Größe (cm)", 
     ylab="Gewicht (Kg)", 
     xlim=c(150,200), 
     pch=16, 
     data=data)
abline(-108.49,1.02, lwd=2, col="darkgreen")
abline(-207.5873,1.5873, lwd=2, col="hotpink1")
points(174.67,69.67, pch=16, col="darkred")
text(173,72,expression(bar(x) * "," * bar(y)))
text(195,80,"Gewicht gegen Größe", col="darkgreen")
text(185,100,"Größe gegen Gewicht", col="hotpink1")

```

Üblicherweise sind die Regressionsgeraden von $Y$ gegen $X$ und $X$ gegen $Y$ **nicht** gleich, aber sie schneiden sich immer im Mittelpunkt $(\overline x, \overline y)$.

Wenn es eine perfekte lineare Beziehung zwischen den Variablen gibt, dann sind beide Regressionsgeraden gleich, da diese Linie sowohl $X$-Residuen als auch $Y$-Residuen auf null setzt.


```{r}
#| label: perfekteRegression
x=runif(100)
y = 0.7*x +0.15
par(mai=c(0.3,0.3,0.4,0.2), 
    mgp=c(0.5,0,0))
plot(x,y, 
     ylim=c(0,1), 
     col="steelblue", 
     main="Perfekte lineare Regression", 
     xlab="X", 
     ylab="Y", 
     pch=16, 
     axes=F)
box()
modelo = lm(y~x)
abline(modelo, lwd=2)
text(0.7,0.4, "Y gegen X = X gegen Y")

```


Wenn es keine lineare Beziehung zwischen den Variablen gibt, dann sind beide Regressionsgeraden konstant und gleich den jeweiligen  Mittelwerten 
$$
y = \bar y, \qquad x = \bar x.
$$


Daher schneiden sie sich senkrecht.

```{r}
#| label: nichtlineareRegression
x=runif(100)
y=runif(100)
par(mai=c(0.3,0.3,0.4,0.2), 
    mgp=c(0.5,0,0))
plot(x,y, 
     ylim=c(0,1), 
     col="skyblue", 
     main="Nicht-lineare Regression", 
     xlab="X", 
     ylab="Y", 
     pch=16, 
     axes=F)
box()
abline(h=mean(y), lwd=2, col="darkgreen")
abline(v=mean(x), lwd=2, col="hotpink3")
text(mean(x)+0.08,0.12, "X gegen Y", col="hotpink3")
text(0.9,mean(y)-0.07, "Y gegen X", col="darkgreen")

```


### Regressionskoeffizient


Der wichtigste Parameter einer Regressionsgeraden  ist die Steigung.

::: {.callout-note appearance="default"}
## Definition Regressionskoeffizient  $b_{xy}$

Für eine zweidimensionale Variable $(X, Y)$ gibt der Regressionskoeffizient der 
Regressionsgeraden von $Y$ gegen $X$ deren Steigung an,

$$
b_{yx} = \frac{s_{xy}}{s_x^2}
$$
:::


Der Regressionskoeffizient hat immer das gleiche Vorzeichen wie die Kovarianz. Er misst, wie sich die abhängige Variable $Y$ in Bezug auf die unabhängige Variable $X$ gemäß der Regressionsgerade verändert. Insbesondere gibt er an, um wie viele Einheiten die abhängige Variable pro Einheit, die die unabhängige Variable zunimmt, steigt oder fällt.

::: {.callout-tip appearance="default"}
## Beispiel: Körpergrößen und Gewichte

In unserer Stichprobe mit den Variablen Körpergröße und Gewicht war die Regressionsgerade für das Gewicht in Bezug auf die Körpergröße

$$
y=-108.49 +1.02 x.
$$

Daher ist der Regressionskoeffizient für das Gewicht in Bezug auf die Körpergröße

$$
b_{yx}= 1.02 \text{ Kg/cm.}
$$


Das bedeutet, dass das Gewicht gemäß der Regressionsgerade in Bezug auf die Körpergröße um 1.02 kg pro Zentimeter zunimmt, wenn die Körpergröße steigt.


```{r}
#| label: slope1
#| engine: tikz
#| fig-pos: h
\definecolor{color2}{rgb}{0.75, 0.1, 0.1}
\begin{tikzpicture}
\draw[color=color2] (0,0) -- (2,2.04);
\draw (0,0) -- (2,0) -- (2,2.04);
\node[align=center] at (-0.7,1.2) {Regressionsgerade\\ Gewicht gegen Größe};
\node[anchor=north] at (1,0) {1 cm};
\node at (1,-0.7){Größe};
\node[anchor=west] at (2,1.02) {1.02 Kg  Gewicht};
\end{tikzpicture}
```

:::



### Modellvorhersage

Üblicherweise werden Regressionsmodelle verwendet, um die abhängige Variable $Y$ für bestimmte Werte der unabhängigen Variablen  $X$ vorherzusagen.

::: {.callout-caution appearance="default"}
## Denken Sie daran! 

Um die besten Vorhersagen einer Variablen zu erhalten, müssen Sie die Regressionsgerade verwenden, bei der diese Variable die abhängige Rolle $Y$ spielt.
:::

::: {.callout-tip appearance="simple"}

Möchten wir das Gewicht einer Person mit einer Körpergröße von 180cm vorhersagen, müssen wir die Regressionsgerade für das Gewicht in Bezug auf die Körpergröße verwenden.

$$
y = -108.49 + 1.02 \cdot 180  = 75.11 \text{ Kg}.
$$


Um hingegen die Körpergröße einer Person mit einem Gewicht von 79kg vorherzusagen, müssen wir die Regressionsgerade für die Körpergröße in Bezug auf das Gewicht verwenden,

$$
x = 130.78 + 0.63\cdot 79 = 180.55 \text{ cm}.
$$
:::


Aber wie zuverlässig sind diese Vorhersagen?





## Korrelation







