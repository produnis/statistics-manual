---
title: Wahrscheinlichkeit
format:
  html:
    include-in-header: 
      text: |
        <img src="https://vg08.met.vgwort.de/na/488822d57f92491f886e46fbf6d962fb" width="1" height="1" alt="">
---

Die deskriptive Statistik bietet Methoden zur Beschreibung der in der Probe gemessenen Variablen und ihrer Beziehungen, erm√∂glicht jedoch  keine Schl√ºsse aus dieser Probe auf die Grundgesamtheit.

Jetzt ist es Zeit, von der Stichprobe zur Grundgesamtheit zu gelangen, und die Br√ºcke daf√ºr ist die **Wahrscheinlichkeitstheorie**.

Bitte beachten Sie, dass die Stichprobe nur begrenzte Informationen √ºber die Grundgesamtheit hat, und um g√ºltige Schl√ºsse f√ºr diese zu ziehen, muss die Probe *repr√§sentativ* sein. Dies ist strenggenommen nur bei Zufallsstichproben gegeben.

Die Wahrscheinlichkeitstheorie wird uns Werkzeuge bieten, um den Zufall in der Stichprobenziehung zu kontrollieren und das Vertrauensniveau der aus der Stichprobe gezogenen Schl√ºsse zu bestimmen.

## Experiment mit zuf√§lligem Ergebnis

Die Untersuchung einer Eigenschaft der Grundgesamtheit erfolgt durch Experimente mit zuf√§lligem Ergebnis.

::: {.callout-warning appearance="default"}
## Definition  "Zufallsexperiment"

Ein Zufallsexperiment ist ein Experiment, das zwei Bedingungen erf√ºllt:

1. Alle m√∂glichen Ergebnisse sind bekannt.
2. Es ist unm√∂glich, das Ergebnis mit absoluter Sicherheit vorherzusagen.

:::

::: {.callout-tip appearance="default"}
## Beispiel: Gl√ºcksspiele

Gl√ºcksspiele sind typische Beispiele f√ºr Experimente mit zuf√§lligem Ergebnis. Zum Beispiel ist das Werfen eines W√ºrfels ein  solches Experiment, weil:

1. Der Satz der m√∂glichen Ergebnisse bekannt ist: $\{1, 2, 3, 4, 5, 6\}$.
2. Bevor der W√ºrfel geworfen wird ist es unm√∂glich das Ergebnis mit absoluter Sicherheit vorherzusagen.

:::


Ein weiteres Beispiel, das nichts mit Gl√ºcksspielen zu tun hat, ist die zuf√§llige Auswahl einer Person aus einer menschlichen Grundgesamtheit und die Bestimmung ihrer Blutgruppe.

Grunds√§tzlich ist die Ziehung einer Stichprobe mittels einer Zufallsmethode ein Zufallsexperiment.

### Wahrscheinlichkeitsraum

::: {.callout-warning appearance="default"}
## Definition "Wahrscheinlichkeitsraum $\Omega$" 
Der Satz $\Omega$ der m√∂glichen Ergebnisse eines Zufallsexperiments wird als Wahrscheinlichkeitsraum (Probabilit√§tsraum) bezeichnet.
:::

::: {.callout-tip appearance="default"}
## Beispiele f√ºr Probabilit√§tsr√§ume:

- Beim Werfen einer M√ºnze ist $\Omega =  \{Kopf, Zahl\}$.
- Beim Werfen eines W√ºrfels ist $\Omega =  \{1, 2, 3, 4, 5, 6 \}$.
- Beim Blutgruppentest einer zuf√§llig ausgew√§hlten Person ist $\Omega =  \{0, A, B, AB \}$.
- Bei der K√∂rpergr√∂√üe einer zuf√§llig ausgew√§hlten Person ist $\Omega=\mathbb{R}^+$.

:::

### Baumdiagramme

In Experimenten, bei denen mehr als eine Variable gemessen wird, kann die Bestimmung des Wahrscheinlichkeitsraums schwierig sein. In solchen F√§llen ist es ratsam, ein Baumdiagramm zur Konstruktion des Probabilit√§tsraums zu verwenden.

In einem Baumdiagramm wird jede Variable auf einer Ebene des Baums dargestellt und jeder m√∂gliche Wert der Variablen als Zweig.

```{r}
#| label: baumdiagrammtik
#| engine: tikz
\begin{tikzpicture}[
grow'=right,
level 1/.style ={level distance=2cm, sibling distance=3.2cm, parent anchor=east, child anchor=west},
level 2/.style ={level distance=4cm, sibling distance=0.8cm},
level 3/.style ={level distance=2cm, sibling distance=0.8cm, dashed}
]

% Hauptbaum
\node (root) {}
	child {
		node (weiblich) {weiblich}
    	child {node (f0) {0}
				child {node{(weiblich,0)}}
			}
 			child {node (fA) {A}
				child {node{(weiblich,A)}}
			}
			child {node (fB) {B}
				child {node{(weiblich,B)}}
			}
			child {node (fAB) {AB}
				child {node{(weiblich,AB)}}
			}
    }
	child {
		node (m√§nnlich) {m√§nnlich}
    	child {node (m0) {0}
				child {node{(m√§nnlich,0)}}
			}
 			child {node (mA) {A}
				child {node{(m√§nnlich,A)}}
			}
			child {node (mB) {B}
				child {node{(m√§nnlich,B)}}
			}
			child {node (mAB) {AB}
				child {node{(m√§nnlich,AB)}}
			}
    };

% Manuelle Labels (ohne scope)
\node[font=\bfseries, text width=2cm, align=center] at ([yshift=1cm]root) {$\Omega$};
\node[text width=2cm, align=center] at ([yshift=1cm]weiblich) {Geschlecht};
\node[text width=2cm, align=center] at ([yshift=1cm]f0) {Blutgruppe};

\end{tikzpicture}

```




### Zufallsereignis

::: {.callout-warning appearance="default"}
## Definition Zufallsereignis

Ein zuf√§lliges Ereignis ist jede Teilmenge des Probabilit√§tsraums $\Omega$ eines Zufallsexperimentes.

:::


Es gibt verschiedene Arten von Ereignissen:

- unm√∂gliches Ereignis: Dies ist das Ereignis ohne Elemente  $\emptyset$. Es hat keine Chance einzutreten.
- elementare Ereignisse: Dies sind Ereignisse mit nur einem Element.
- zusammengesetzte Ereignisse: Dies sind Ereignisse mit zwei oder mehr Elementen.
- sicheres Ereignis: Dies ist das Ereignis, das den gesamten Probabilit√§tsraum $\Omega$ enth√§lt. Es tritt immer ein.

## Mengentheorie

### Ereignisraum

::: {.callout-warning appearance="default"}
##  Definition 32 Ereignisraum

Gegeben einen Wahrscheinlichkeitsraum $\Omega$ eines Zufallsexperimentes, ist die Ereignismenge von $\Omega$ die Menge aller m√∂glichen Ereignisse von $\Omega$ und wird mit $\mathcal{P}(\Omega)$ bezeichnet.
:::

::: {.callout-tip appearance="default"}
## Beispiel

F√ºr den Probabilit√§tsraum $\Omega=\{a,b,c\}$ ist dessen Ereignismenge:

$$
\mathcal{P}(\Omega)=\left\{\emptyset, \{a\},\{b\},\{c\},\{a,b\},\{a,c\},\{b,c\},\{a,b,c\}\right\}
$$
:::


### Ereignisoperationen

Da Ereignisse Teilmengen des Wahrscheinlichkeitsraums sind, haben wir mittels der Mengentheorie folgende Operationen an Ereignissen:

- Vereinigung
- Schnittmenge
- Komplement√§rmenge
- Differenzmenge

#### Vereinigung von Ereignissen

::: {.callout-warning appearance="default"}
## Definition "Vereinigungsereignis"

Gegeben zwei Ereignisse $A,B\subseteq \Omega$, ist die Vereinigung von $A$ und $B$, bezeichnet mit $A\cup B$, das Ereignis aller Elemente, die Mitglieder von $A$ oder $B$ oder beiden sind.

$$
A\cup B = \{x\,|\, x\in A\textrm{ oder }x\in B\}.
$$
:::


```{r}
#| engine: tikz
#| label: vereinigungsereignis
\begin{tikzpicture}
\def\firstcircle{(1.5,1.5) circle (1cm)}
\def\secondcircle{(2.5,1.5) circle (1cm)}

\fill[blue!30] \firstcircle;
\fill[blue!30] \secondcircle;
\draw (0,3) node[anchor=north east] {$\Omega$} rectangle (4,0);
\draw \firstcircle node[xshift=-0.9cm, yshift=0.9cm] {$A$};
\draw \secondcircle node[xshift=0.9cm, yshift=0.9cm] {$B$};

\node at (2,0.3) {$A\cup B$};
\end{tikzpicture}
```

Das Vereinigungsereignis  $A,B\subseteq \Omega$  tritt ein, wenn $A$ oder $B$ eintreten.



#### Schnittereignis

::: {.callout-warning appearance="default"}
## Definition "Schnittereignis"

Angenommen, wir haben zwei Ereignisse $A$ und $B$, die Teilmengen von $\Omega$ sind. Das Schnittereignis (*Intersection*) von $A$ und $B$, bezeichnet mit $A\cap B$, ist das Ereignis aller Elemente, die sowohl zu $A$ als auch zu $B$ geh√∂ren.

$$
A\cap B = \{x\,|\, x\in A\mbox{ und }x\in B\}.
$$
:::

```{r}
#| engine: tikz
#| label: schnittereignis
\begin{tikzpicture}
\def\firstcircle{(1.5,1.5) circle (1cm)}
\def\secondcircle{(2.5,1.5) circle (1cm)}

\begin{scope}
\clip \firstcircle;
\fill[blue!30] \secondcircle;
\end{scope}

\draw (0,3) node[anchor=north east] {$\Omega$} rectangle (4,0);
\draw \firstcircle node[xshift=-0.9cm, yshift=0.9cm] {$A$};
\draw \secondcircle node[xshift=0.9cm, yshift=0.9cm] {$B$};

\node at (2,1.5) {$A\cap B$};
\end{tikzpicture}
```


Das Ereignis $A\cap B$ tritt ein, wenn sowohl $A$ als auch $B$ eintreten.
Zwei Ereignisse sind **unm√∂glich**, wenn ihr Schnitt leer ist.


#### Komplement√§rereignis

::: {.callout-warning appearance="default"}
## Definition "Komplement√§rereignis"

Angenommen, wir haben ein Ereignis $A$, das eine Teilmenge von $\Omega$ ist. Das komplement√§re oder gegens√§tzliche Ereignis zu $A$, bezeichnet mit $\overline A$, ist das Ereignis aller Elemente in $\Omega$ au√üer denjenigen, die zu $A$ geh√∂ren.

$$
\overline A = \{x\,|\, x\not\in A\}.
$$
:::

```{r}
#| engine: tikz
#| label: komplementaer
\begin{tikzpicture}
\def\circle{(1.5,1.5) circle (1cm)}
\def\rectangle{(4,0) rectangle (0,3)}

\begin{scope}[even odd rule]
\clip \circle (0,0) rectangle (4,3);
\fill[blue!30] \rectangle;
\end{scope}

\draw \rectangle node[anchor=north east] {$\Omega$};
\draw \circle node {$A$};
\node at (3,1.5) {$\overline A$};
\end{tikzpicture}
```

Das komplement√§re Ereignis $\overline A$ tritt ein, wenn $A$ nicht eintritt.


#### Differenzereignis

::: {.callout-warning appearance="default"}
## Definition "Differenzereignis"

Gegeben sind zwei Ereignisse $A$ und $B$ als Teilmengen von $\Omega$. Die Differenz von $A$ und $B$, bezeichnet mit $A-B$, ist das Ereignis aller  Elemente, die zu $A$ geh√∂ren, aber nicht zu $B$.


$$
A-B = \{x\,|\, x\in A\mbox{ und }x\not\in B\} = A \cap \overline B.
$$
:::

```{r}
#| label: differenzereignis
#| engine: tikz
\begin{tikzpicture}
\def\firstcircle{(1.5,1.5) circle (1cm)}
\def\secondcircle{(2.5,1.5) circle (1cm)}

\begin{scope}[even odd rule]
\clip \secondcircle (0,0) rectangle (4,3);
\fill[blue!30] \firstcircle;
\end{scope}

\draw (0,3) node[anchor=north east] {$\Omega$} rectangle (4,0);
\draw \firstcircle node[xshift=-0.9cm, yshift=0.9cm] {$A$};
\draw \secondcircle node[xshift=0.9cm, yshift=0.9cm] {$B$};

\node[anchor=east] at (1.5,1.5) {$A-B$};
\end{tikzpicture}
```


Das Differenzereignis $A-B$ tritt ein, wenn $A$ eintritt, aber $B$ nicht.


#### Beispiel

::: {.callout-tip appearance="default"}
## Beispiel 

Der Ereignisraum beim W√ºrfeln ist $\Omega=\{1,2,3,4,5,6\}$ und die Ereignisse sind 

- $A=\{2,4,6\}$ und 
- $B=\{1,2,3,4\}$, 

dann gilt:

- Die Vereinigungsmenge von $A$ und $B$ ist $A\cup B=\{1,2,3,4,6\}$.
- Die Schnittmenge von  $A$ und $B$ ist $A\cap B=\{2,4\}$.
- Das Komplement von $A$ ist $\overline A=\{1,3,5\}$
- Die Ereignisse $A$ und $\overline A$ schlie√üen sich gegenseitig aus.
- Die Differenz von $A$ und $B$ ist $A-B=\{6\}$ und die Differenz von $B$ und $A$ ist $B-A=\{1,3\}$
:::


### Algebra

Sind die Ereignisse $A,B,C\subseteq \Omega$ gegeben, treffen die folgenden algebra'schen Eigenschaften zu:

- **Idempotenz:** $A\cup A=A,\quad A\cap A=A$ 
- **Kommutativit√§t:**  $A\cup B=B\cup A,\quad A\cap B = B\cap A$
- **Assoziativit√§t:**  $(A\cup B)\cup C = A\cup (B\cup C),\quad (A\cap B)\cap C = A\cap (B\cap C)$
- **Distributivit√§t:** $(A\cup B)\cap C = (A\cap C)\cup (B\cap C),\quad (A\cap B)\cup C = (A\cup C)\cap (B\cup C)$
- **Neutrales Element:**  $A\cup \emptyset=A,\quad A\cap \Omega=A$
- **absorbierendes Element:**  $A\cup \Omega=\Omega,\quad A\cap \emptyset=\emptyset$
- **komplement√§res symmetrisches Element** $A\cup \overline A = \Omega,\quad A\cap \overline A= \emptyset$ 
- **doppelte Negation:**  $\overline{\overline A} = A$
- **De Morgansche Gesetze:** $\overline{A\cup B} = \overline A\cap \overline B,\quad \overline{A\cap B} = \overline A\cup \overline B$






## Wahrscheinlichkeit

::: {.callout-warning appearance="default"}
## Defintion "Wahrscheinlichkeit" (nach Laplace)
F√ºr ein Zufallsexperiment mit einem Ereignisraum $\Omega$, dessen Elemente alle gleich wahrscheinlich 
sind, ist die Wahrscheinlichkeit eines Ereignisses $A\subseteq \Omega$ der Quotient zwischen der Anzahl der Elemente von $A$ und der Anzahl der Elemente von $\Omega$

$$
P(A) = \frac{|A|}{|\Omega|} = \frac{\mbox{Anzahl gew√ºnschter Ergebnisse}}{\mbox{Anzahl aller m√∂glichen Ergebnisse}}
$$
:::

Diese Definition ist gut bekannt, hat aber wichtige Einschr√§nkungen:

- Es wird verlangt, dass alle Elemente des Ereignisraums gleich wahrscheinlich sind (Gleichwahrscheinlichkeit).
- Sie kann nicht bei unendlichen gro√üen Ereignisr√§umen angewendet werden.

::: {.callout-important appearance="default"}
## Vorsicht! 
Diese Bedingungen werden in vielen echten Experimenten nicht erf√ºllt.
:::



::: {.callout-tip appearance="default"}
## Beispiel
Angenommen, der Ereignisraum beim Werfen eines W√ºrfels ist $\Omega=\{1,2,3,4,5,6\}$ und das Ereignis $A=\{2,4,6\}$, dann betr√§gt die Wahrscheinlichkeit von $A$

$$
P(A) = \frac{|A|}{|\Omega|} = \frac{3}{6} = 0.5.
$$

Allerdings ist es beim Ereignisraum der Blutgruppe einer zuf√§llig ausgew√§hlten Person $\Omega=\{O,A,B,AB\}$ nicht m√∂glich, die klassische Definition zu verwenden, um die Wahrscheinlichkeit f√ºr Gruppe $A$ zu berechnen,

$$
P(A) \neq \frac{|A|}{|\Omega|} = \frac{1}{4} = 0.25,
$$

...weil Blutgruppen in der menschlichen Bev√∂lkerung nicht *gleich* wahrscheinlich sind.

:::



### H√§ufigkeitswahrscheinlichkeit

::: {.callout-warning appearance="default"}
## Definition "Gesetz der gro√üen Zahlen"
Wenn ein zuf√§lliges Experiment eine gro√üe Anzahl von Malen wiederholt wird, n√§hert sich die relative 
H√§ufigkeit eines Ereignisses der Wahrscheinlichkeit des Ereignisses an.

:::

Die folgende Definition der Wahrscheinlichkeit basiert auf diesem Satz.

::: {.callout-warning appearance="default"}
## Definition "Frequenzwahrscheinlichkeit"
Gegeben ein Ereignisraum $\Omega$ eines reproduzierbaren zuf√§lligen Experiments, betr√§gt die 
Wahrscheinlichkeit eines Ereignisses $A\subseteq \Omega$ die relative H√§ufigkeit des Ereignisses $A$ bei einer unbegrenzten Anzahl von Wiederholungen des Experiments.

$$
P(A) = lim_{n\rightarrow \infty}\frac{n_A}{n}
$$
:::



Auch diese Definition hat einige Nachteile:

- Sie berechnet eine *Sch√§tzung* der realen Wahrscheinlichkeit.
- Die Wiederholung des Experiments muss unter identischen Bedingungen stattfinden.

::: {.callout-tip appearance="default"}
## Beispiel M√ºnzwurf
Gegeben ist der Ereignisraum beim Werfen einer M√ºnze $\Omega=\{Kopf, Zahl\}$. 
Wenn nach $100$ W√ºrfen 54mal Kopf herauskam, betr√§gt die Wahrscheinlichkeit von $Kopf$
$$
P(Kopf) = \frac{n_{Kopf}}{n} = \frac{54}{100} = 0.54.
$$

:::

::: {.callout-tip appearance="default"}
## Beispiel Blutgruppen
Gegeben ist der Ereignisraum der Blutgruppe einer zuf√§llig ausgew√§hlten Person $\Omega=\{O,A,B,AB\}$. Wenn nach einer Zufallsstichprobe von 1.000  Personen 412 mit der Blutgruppe $A$ dabei waren, betr√§gt die Wahrscheinlichkeit von $A$

$$
P(A) = \frac{n_A}{n} = \frac{412}{1000} = 0.412.
$$
:::




### Axiomatische Wahrscheinlichkeit

::: {.callout-warning appearance="default"}
## Definition "Wahrscheinlichkeit" (nach Kolmog√≥rov)
Gegeben ein Ereignisraum $\Omega$ eines zuf√§lligen Experiments, ist eine  Wahrscheinlichkeitsfunktion eine Funktion, die jeden Ereigniswert $A\subseteq \Omega$ auf eine reelle Zahl $P(A)$ (bekannt als Wahrscheinlichkeit von $A$), abbildet und folgende Axiome erf√ºllt:

1. Die Wahrscheinlichkeit jedes Ereignisses ist nicht negativ, 
$$
P(A)\geq 0.
$$
2. Die Wahrscheinlichkeit des Ereignisraums betr√§gt 1,
$$
P(\Omega)=1
$$
3. Die Wahrscheinlichkeit der Vereinigung zweier inkompatibler Ereignisse ($A\cap B=\emptyset$) ist die Summe ihrer Wahrscheinlichkeiten
$$
P(A\cup B) = P(A)+P(B).
$$
:::


#### Eigenschaften der axiomatischen Wahrscheinlichkeit

Aus diesen Axiomen lassen sich einige wichtige Eigenschaften einer Wahrscheinlichkeitsfunktion ableiten.
Gegeben ein Ereignisruam $\Omega$ eines zuf√§lligen Experiments und die Ereignisse $A,B\subseteq \Omega$, gelten folgende Eigenschaften:

1. $P(\overline A) = 1-P(A)$.
2. $P(\emptyset)= 0$.
3. Wenn $\ A\subseteq B\ $ dann $\ P(A)\leq P(B)$.
4. $P(A) \leq 1\ $. Das bedeutet $\ P(A)\in [0,1]$.
5. $P(A-B)=P(A)-P(A\cap B)$. 
6. $P(A\cup B)= P(A) + P(B) - P(A\cap B)$.
7. Wenn $\ A=\{e_1,\ldots,e_n\}\ $, und $\ e_i$ $i=1,\ldots,n\ $ Elementarereignisse sind, dann gilt
$$
P(A)=\sum_{i=1}^n P(e_i).
$$





### Wahrscheinlichkeitsinterpretation

Gem√§√ü den vorherigen Axiomen ist die Wahrscheinlichkeit eines Ereignisses $A$ eine reelle Zahl $P(A)$, die stets im Bereich von 0 bis 1  liegt.

Auf gewisse Weise dr√ºckt diese Zahl die Glaubw√ºrdigkeit des Ereignisses aus, d.h. die Chancen, dass das Ereignis $A$ in dem Experiment  eintritt. Daher gibt sie auch ein Ma√ü f√ºr die *Unsicherheit* bez√ºglich des Ereignisses an.

- Die maximale Unsicherheit entspricht der Wahrscheinlichkeit $P(A) = 0,5\quad$ ($A$ und $\overline A$ haben die gleiche Chance, einzutreten).
- Die minimale Unsicherheit entspricht der Wahrscheinlichkeit $P(A) = 1\quad$ ($A$ wird mit absoluter Sicherheit eintreten) und $P(A) = 0\quad$ ($A$ wird  mit absoluter Sicherheit nicht eintreten). 

Wenn $P(A)$ n√§her bei $0$ als bei $1$ liegt, sind die Chancen, dass $A$ nicht eintritt, gr√∂√üer als die 
Chancen, dass $A$ eintritt. 

Im Gegensatz dazu, wenn $P(A)$ n√§her bei $1$ als bei $0$ liegt, sind die Chancen, dass $A$ eintritt, gr√∂√üer als die Chancen, dass $A$ nicht eintritt.









## bedingte Wahrscheinlichkeit 

Gelegentlich k√∂nnen wir bereits vor der Durchf√ºhrung eines Experiments einige Informationen dar√ºber erhalten. √úblicherweise wird diese Information als Ereignis $B$ des gleichen Ereignisraums gegeben, von dem wir wissen, dass es wahr ist, bevor wir das Experiment durchf√ºhren.

In einem solchen Fall werden wir sagen, dass $B$ ein *konditionierendes Ereignis* ist und die Wahrscheinlichkeit eines anderen Ereignisses $A$ als *bedingte Wahrscheinlichkeit* bezeichnen. Diese wird  als $P(A|B)$ ausgedr√ºckt.

Dies muss als "Wahrscheinlichkeit von $A$ unter der Bedingung $B$" oder "Wahrscheinlichkeit von $A$ gegeben $B$" gelesen werden.

::: {.callout-tip appearance="default"}
## Beispiel

Konditionierende Ereignisse √§ndern normalerweise den Ereignisraum und damit die Wahrscheinlichkeiten der Ereignisse. Angenommen, wir haben eine Stichprobe von 100 Frauen und 100 M√§nnern mit den folgenden H√§ufigkeiten:

$$
\begin{array}{|c|c|c|}
 & \mbox{Nichtraucher} & \mbox{Raucher} \\ \hline
 \rowcolor{coral}\color{white} \mbox{Frauen} & \color{white}80 & \color{white}20 \\ \hline
 \mbox{M√§nner} & 60 & 40 \\ \hline
\end{array}
$$

Dann betr√§gt die Wahrscheinlichkeit, ein Raucher zu sein, basierend auf der gesamten Probe:

$$
P(\mbox{Raucher})= \frac{60}{200}=0.3.
$$

Wenn wir jedoch wissen, dass die Person eine Frau ist, wird die Stichprobe auf die erste Zeile reduziert und die Wahrscheinlichkeit, eine Raucherin zu sein, betr√§gt:

$$
P(\mbox{Raucherin}|\mbox{Frau})=\frac{20}{100}=0.2.
$$

:::




::: {.callout-warning appearance="default"}
## Definition "bedingte Wahrscheinlichkeit"
Gegeben ist ein Ereignisraum $\Omega$ eines zuf√§lligen Experiments und zwei Ereignisse $A,B\subseteq \Omega$, so ist die Wahrscheinlichkeit von $A$ unter der Bedingung, dass $B$ eintritt:
$$
P(A|B) = \frac{P(A\cap B)}{P(B)},
$$
solange $P(B)\neq 0$.
:::

Diese Definition erm√∂glicht es, bedingte Wahrscheinlichkeiten zu berechnen, ohne den urspr√ºnglichen Ereignisraum zu √§ndern.

::: {.callout-tip appearance="default"}
## Beispiel
In dem vorherigen Beispiel ist die bedingte Wahrscheinlichkeit, dass eine Person Raucher und weiblich ist:

$$
P(\mbox{Raucher}|\mbox{weiblich})= \frac{P(\mbox{Raucher}\cap \mbox{weiblich})}{P(\mbox{weiblich})} = \frac{20/200}{100/200}=\frac{20}{100}=0.2.
$$
:::




### Wahrscheinlichkeit des Schnittmengen-Ereignisses:

Aus der Definition der bedingten Wahrscheinlichkeit kann die Formel f√ºr die Wahrscheinlichkeit der Schnittmenge zweier Ereignisse abgeleitet werden:

$$
P(A\cap B) = P(A)P(B|A) = P(B)P(A|B).
$$


::: {.callout-tip appearance="default"}
## Beispiel: Brustkrebs 
In einer Bev√∂lkerung gibt es 30 % Raucherinnen und wir wissen, dass 40 % dieser Raucher Krebs haben. Die Wahrscheinlichkeit, dass eine zuf√§llig ausgew√§hlte Person raucht und Krebs hat, betr√§gt:

$$
P(\mbox{Raucher}\cap \mbox{Krebs})= P(\mbox{raucher})P(\mbox{Krebs}|\mbox{Raucher}) =
0.3\times 0.4 = 0.12.
$$
:::





### Unabh√§ngigkeit der Ereignisse

Manchmal √§ndert das bedingende Ereignis die urspr√ºngliche Wahrscheinlichkeit des Hauptereignisses nicht.

::: {.callout-warning appearance="default"}
## Definition "Unabh√§ngige Ereignisse"
Gegeben einen Ereignisraum $\Omega$ eines zuf√§lligen Experiments, sind zwei Ereignisse $A,B\subseteq \Omega$ *unabh√§ngig*, wenn die Wahrscheinlichkeit von $A$ sich nicht √§ndert, wenn sie durch $B$ bedingt ist, und umgekehrt, d.h.,

$$
P(A|B) = P(A) \quad \mbox{und} \quad P(B|A)=P(B),
$$
wenn $P(A)\neq 0\ $ und $\ P(B)\neq 0$.
:::


Das bedeutet, dass das Eintreten eines Ereignisses keine relevante Information liefert, um die Unsicherheit des anderen Ereignisses zu √§ndern.

Wenn zwei Ereignisse unabh√§ngig sind, ist die Wahrscheinlichkeit ihrer Schnittmenge gleich dem Produkt ihrer Wahrscheinlichkeiten,

$$
P(A\cap B) = P(A)\cdot P(B)
$$


::: {.callout-tip appearance="default"}
## Beispiel: M√ºnzwurf

Der Ereignisraum f√ºr den doppelten Wurf einer M√ºnze ist $\Omega=\{(K,K),(K,Z),(Z,K),(Z,Z)\}$ und alle Elemente sind gleich wahrscheinlich, wenn die M√ºnze fair ist. Daher haben wir durch die klassische Definition der Wahrscheinlichkeit

$$
P((K,K)) = \frac{1}{4} = 0.25.
$$
Wenn wir $K_1=\{(K,K),(K,Z)\}$, d.h., Kopf beim ersten Wurf, und $H_2=\{(K,K),(Z,K)\}$ nennen, d.h., Kopf beim zweiten Wurf, k√∂nnen wir das gleiche Ergebnis erhalten, indem wir davon ausgehen, dass diese Ereignisse unabh√§ngig sind,

$$
P(K,K)= P(K_1\cap K_2) = P(K_1)\cdot P(K_2) = \frac{2}{4}\cdot\frac{2}{4}=\frac{1}{4}=0.25.
$$
:::







## Wahrscheinlichkeitsraum

::: {.callout-warning appearance="default"}
## Definition "Wahrscheinlichkeitsraum"

Ein Wahrscheinlichkeitsraum eines zuf√§lligen Experiments ist ein Tripel $(\Omega,\mathcal{F},P)$ 
wobei

- $\Omega\ $ der Ereignisraum des Experiments ist.
- $\mathcal{F}\ $  eine Menge von Ereignissen des Experiments ist.
- $P\ $  eine Wahrscheinlichkeitsfunktion ist.

:::

Wenn wir die Wahrscheinlichkeiten aller Elemente von $\Omega$ kennen, k√∂nnen wir leicht den Wahrscheinlichkeitsraum konstruieren und die Wahrscheinlichkeit jedes Ereignisses in $\mathcal{F}$ berechnen.





### Konstruktion des Wahrscheinlichkeitsraums

Um die Wahrscheinlichkeit jedes m√∂glichen (elementaren) Ereignisses zu berechnen, k√∂nnen wir ein Baumdiagramm verwenden. Dabei gelten folgende Regeln:

1. Jede Kante im Baum erh√§lt eine Wahrscheinlichkeit. Diese gibt an, wie wahrscheinlich der jeweilige Wert unter der Bedingung der vorherigen Knoten ist.
2.  Die Wahrscheinlichkeit eines vollst√§ndigen Pfads (also eines Ereignisses ganz am Ende des Baums) ergibt sich, indem man die Wahrscheinlichkeiten aller Kanten entlang dieses Pfads miteinander multipliziert ‚Äì vom Start (Wurzel) bis zum Ende (Blatt).

```{r}
#| engine: tikz
#| label: probTree1
\begin{tikzpicture}[
  grow'=right,
  level 1/.style ={level distance=2cm, sibling distance=1.6cm, parent anchor=east, child anchor=west},
  level 2/.style ={level distance=2.5cm, sibling distance=0.8cm},
  level 3/.style ={level distance=1.5cm, sibling distance=0.8cm, dashed},
  level 4/.style ={level distance=3.5cm, sibling distance=0.8cm, dashed},
  prob/.style={font=\footnotesize,above}
]

% Baumstruktur
\node (root) {}
  child {node (x1) {$x_1$}
    child {node (y1x1) {$y_1$}
      child {node (xy1x1) {$(x_1,y_1)$} 
        child {node (pxy1x1) {$P(x_1\cap y_1)$} edge from parent node[prob] {$P(x_1)P(y_1|x_1)$}}
      }
      edge from parent node[prob] {$P(y_1|x_1)$}
    }
    child {node (y2x1) {$y_2$}
      child {node (xy2x1) {$(x_1,y_2)$} 
        child {node (pxy2x1) {$P(x_1\cap y_2)$} edge from parent node[prob] {$P(x_1)P(y_2|x_1)$}}
      }
      edge from parent node[prob,below] {$P(y_2|x_1)$}
    }
    edge from parent node[prob] {$P(x_1)$}
  }
  child {node (x2) {$x_2$}
    child {node (y1x2) {$y_1$}
      child {node (xy1x2) {$(x_2,y_1)$} 
        child {node (pxy1x2) {$P(x_2\cap y_1)$} edge from parent node[prob] {$P(x_2)P(y_1|x_2)$}}
      }
      edge from parent node[prob] {$P(y_1|x_2)$}
    }
    child {node (y2x2) {$y_2$}
      child {node (xy2x2) {$(x_2,y_2)$}
        child {node (pxy2x2) {$P(x_2\cap y_2)$} edge from parent node[prob] {$P(x_2)P(y_2|x_2)$}}
      }
      edge from parent node[prob,below] {$P(y_2|x_2)$}
    }
    edge from parent node[prob,below] {$P(x_2)$}
  };

% Beschriftung √ºber den Ebenen (ohne scope)
\node[text width=3.5cm, align=center] at ([yshift=1cm]pxy1x1.north) {Wahrscheinlichkeit};
\node[font=\bfseries, text width=2cm, align=center] at ([yshift=1cm]xy1x1.north) {$\Omega$};
\node[font=\bfseries, text width=2cm, align=center] at ([yshift=1cm]y1x1.north) {$Y$};
\node[font=\bfseries, text width=2cm, align=center] at ([yshift=1cm]x1.north) {$X$};

\end{tikzpicture}

```



### Wahrscheinlichkeitsbaum mit abh√§ngigen Variablen

::: {.callout-tip appearance="default"}
## Beispiel:  Rauchen und Krebs:

In einer Population gibt es 30 % Raucher und wir wissen, dass 40 % der Raucher Krebs haben, w√§hrend nur 10 % der Nichtraucher an Krebs erkrankt sind. Der Wahrscheinlichkeitsbaum des Wahrscheinlichkeitsraums des zuf√§lligen Experiments, das darin besteht, eine zuf√§llig ausgew√§hlte Person auszuw√§hlen und die Variablen Rauchen und Brustkrebs zu messen, ist unten dargestellt.

```{r}
#| label: probTreeRauchen
#| engine: tikz
\begin{tikzpicture}[
  grow'=right,
  level 1/.style ={level distance=2cm, sibling distance=1.6cm, parent anchor=east, child anchor=west},
  level 2/.style ={level distance=2cm, sibling distance=0.8cm},
  level 3/.style ={level distance=1.5cm, sibling distance=0.8cm, dashed},
  level 4/.style ={level distance=3cm, sibling distance=0.8cm, dashed},
  prob/.style={font=\footnotesize,above}
]

\node (root) {}
  child {node (S) {R}
    child {node (C1) {K}
      child {node (SC) {(R,K)}
        child {node (PSC) {$0.12$} edge from parent node[prob] {$0.3\cdot 0.4$}}
      }
      edge from parent node[prob] {$0.4$}
    }
    child {node (nC1) {$\overline{\mbox{K}}$}
      child {node (SnC) {(R,$\overline{\mbox{K}}$)}
        child {node (PSnC) {$0.18$} edge from parent node[prob] {$0.3\cdot 0.6$}}
      }
      edge from parent node[prob,below] {$0.6$}
    }
    edge from parent node[prob] {$0.3$}
  }
  child {node (nS) {$\overline{\mbox{R}}$}
    child {node (C2) {K}
      child {node (nSC) {($\overline{\mbox{R}}$,K)}
        child {node (PnSC) {$0.07$} edge from parent node[prob] {$0.7\cdot 0.1$}}
      }
      edge from parent node[prob] {$0.1$}
    }
    child {node (nC2) {$\overline{\mbox{K}}$}
      child {node (nSnC) {($\overline{\mbox{R}}$,$\overline{\mbox{K}}$)}
        child {node (PnSnC) {$0.63$} edge from parent node[prob] {$0.7\cdot 0.9$}}
      }
      edge from parent node[prob,below] {$0.9$}
    }
    edge from parent node[prob,below] {$0.7$}
  };

% Beschriftung ohne scope
\node[text width=3.5cm, align=center] at ([yshift=1cm]PSC.north) {Wahrscheinlichkeit};
\node[font=\bfseries, text width=2.5cm, align=center] at ([yshift=1cm]SC.north) {$\Omega$};
\node[text width=2.5cm, align=center] at ([yshift=1cm]C1.north) {Krebs};
\node[text width=2.5cm, align=center] at ([yshift=1cm]S.north) {Rauchen};

\end{tikzpicture}

```


:::


### Wahrscheinlichkeitsbaum mit unabh√§ngigen Variablen

::: {.callout-tip appearance="default"}
## Beispiel M√ºnzwurf
Der Wahrscheinlichkeitsbaum f√ºr das Zufallsexperiment zwei M√ºnzen zu werfen ist:

```{r}
#| label: probZreeMuenz
#| engine: tikz
\begin{tikzpicture}[
  grow'=right,
  level 1/.style ={level distance=2cm, sibling distance=1.6cm, parent anchor=east, child anchor=west},
  level 2/.style ={level distance=2cm, sibling distance=0.8cm},
  level 3/.style ={level distance=1.5cm, sibling distance=0.8cm, dashed},
  level 4/.style ={level distance=3cm, sibling distance=0.8cm, dashed},
  prob/.style={font=\footnotesize,above}
]

% Baumstruktur
\node (root) {}
  child {node (K1) {K}
    child {node (K2) {K}
      child {node (KK) {(K,K)}
        child {node (PKK) {$0.25$} edge from parent node[prob] {$0.5\cdot 0.5$}}
      }
      edge from parent node[prob] {$0.5$}
    }
    child {node (KZ) {Z}
      child {node (KZZ) {(K,Z)}
        child {node (PKZ) {$0.25$} edge from parent node[prob] {$0.5\cdot 0.5$}}
      }
      edge from parent node[prob,below] {$0.5$}
    }
    edge from parent node[prob,above] {$0.5$}
  }
  child {node (Z1) {Z}
    child {node (ZK) {K}
      child {node (ZKK) {(Z,K)}
        child {node (PZK) {$0.25$} edge from parent node[prob] {$0.5\cdot 0.5$}}
      }
      edge from parent node[prob] {$0.5$}
    }
    child {node (ZZ) {Z}
      child {node (ZZZ) {(Z,Z)}
        child {node (PZZ) {$0.25$} edge from parent node[prob] {$0.5\cdot 0.5$}}
      }
      edge from parent node[prob,below] {$0.5$}
    }
    edge from parent node[prob,below] {$0.5$}
  };

% Beschriftung ohne scope
\node[text width=3.5cm, align=center] at ([yshift=1cm]PKK.north) {Wahrscheinlichkeit};
\node[font=\bfseries, text width=2.5cm, align=center] at ([yshift=1cm]KK.north) {$\Omega$};
\node[text width=2.5cm, align=center] at ([yshift=1cm]K2.north) {M√ºnze 2};
\node[text width=2.5cm, align=center] at ([yshift=1cm]K1.north) {M√ºnze 1};

\end{tikzpicture}

```

:::


::: {.callout-tip appearance="default"}
## Beispiel mit 3 Variablen
In einer Population gibt es 40% M√§nner und 60% Frauen. Der Wahrscheinlichkeitsbaum f√ºr das Ziehen einer zuf√§lligen Stichprobe von drei Personen ist unten dargestellt.

```{r}
#| engine: tikz
#| label: probTree3Vars
\begin{tikzpicture}[
  grow'=right,
  level 1/.style ={level distance=2cm, sibling distance=3.2cm, parent anchor=east, child anchor=west},
  level 2/.style ={level distance=2cm, sibling distance=1.6cm},
  level 3/.style ={level distance=2cm, sibling distance=0.8cm},
  level 4/.style ={level distance=1.5cm, sibling distance=0.8cm, dashed},
  level 5/.style ={level distance=3cm, sibling distance=0.8cm, dashed},
  prob/.style={font=\footnotesize,above}
]

\node (root) {}
  child {node (P1w) {w}
    child {node (P2w1) {w}
      child {node (P3w1) {w}
        child {node (www) {(w,w,w)}
          child {node (Pwww) {$0{,}216$} edge from parent node[prob] {$0{,}6\cdot 0{,}6\cdot 0{,}6$}}
        }
        edge from parent node[prob] {$0{,}6$}
      }
      child {node (P3m1) {m}
        child {node (wwm) {(w,w,m)}
          child {node (Pwwm) {$0{,}144$} edge from parent node[prob] {$0{,}6\cdot 0{,}6\cdot 0{,}4$}}
        }
        edge from parent node[prob,below] {$0{,}4$}
      }
      edge from parent node[prob] {$0{,}6$}
    }
    child {node (P2m1) {m}
      child {node (P3w2) {w}
        child {node (wmw) {(w,m,w)}
          child {node (Pwmw) {$0{,}144$} edge from parent node[prob] {$0{,}6\cdot 0{,}4\cdot 0{,}6$}}
        }
        edge from parent node[prob] {$0{,}6$}
      }
      child {node (P3m2) {m}
        child {node (wmm) {(w,m,m)}
          child {node (Pwmm) {$0{,}096$} edge from parent node[prob] {$0{,}6\cdot 0{,}4\cdot 0{,}4$}}
        }
        edge from parent node[prob,below] {$0{,}4$}
      }
      edge from parent node[prob,below] {$0{,}4$}
    }
    edge from parent node[prob,left] {$0{,}6$}
  }
  child {node (P1m) {m}
    child {node (P2w2) {w}
      child {node (P3w3) {w}
        child {node (mww) {(m,w,w)}
          child {node (Pmww) {$0{,}144$} edge from parent node[prob] {$0{,}4\cdot 0{,}6\cdot 0{,}6$}}
        }
        edge from parent node[prob] {$0{,}6$}
      }
      child {node (P3m3) {m}
        child {node (mwm) {(m,w,m)}
          child {node (Pmwm) {$0{,}096$} edge from parent node[prob] {$0{,}4\cdot 0{,}6\cdot 0{,}4$}}
        }
        edge from parent node[prob,below] {$0{,}4$}
      }
      edge from parent node[prob] {$0{,}6$}
    }
    child {node (P2m2) {m}
      child {node (P3w4) {w}
        child {node (mmw) {(m,m,w)}
          child {node (Pmmw) {$0{,}096$} edge from parent node[prob] {$0{,}4\cdot 0{,}4\cdot 0{,}6$}}
        }
        edge from parent node[prob] {$0{,}6$}
      }
      child {node (P3m4) {m}
        child {node (mmm) {(m,m,m)}
          child {node (Pmmm) {$0{,}064$} edge from parent node[prob] {$0{,}4\cdot 0{,}4\cdot 0{,}4$}}
        }
        edge from parent node[prob,below] {$0{,}4$}
      }
      edge from parent node[prob,below] {$0{,}4$}
    }
    edge from parent node[prob,left] {$0{,}4$}
  };

% Beschriftungen oben ohne scope
\node[text width=3.5cm, align=center] at ([yshift=1cm]Pwww.north) {Wahrscheinlichkeit};
\node[font=\bfseries, text width=2.5cm, align=center] at ([yshift=1cm]www.north) {$\Omega$};
\node[text width=2.5cm, align=center] at ([yshift=1cm]P3w1.north) {Person 3};
\node[text width=2.5cm, align=center] at ([yshift=1cm]P2w1.north) {Person 2};
\node[text width=2.5cm, align=center] at ([yshift=1cm]P1w.north) {Person 1};

\end{tikzpicture}

```

:::













## Satz von der vollst√§ndigen Wahrscheinlichkeit:

::: {.callout-warning appearance="default"}
##  Definition "Aufteilung des Wahrscheinlichkeitsraums"

Eine Menge von Ereignissen $A_1,A_2,\ldots,A_n$ aus demselben Ereignisraum $\Omega$
hei√üt eine *Aufteilung des Wahrscheinlichkeitsraums*, wenn sie die folgenden Bedingungen erf√ºllt:

1. Die Vereinigung der Ereignisse ist der Ereignisraum, das hei√üt, $A_1\cup \cdots\cup A_n =\Omega$
2. Alle Ereignisse sind paarweise ausschlie√üend, das hei√üt  $A_i\cap A_j = \emptyset$ $\forall i\neq j$.

```{r}
#| label: fullProps
#| engine: tikz
\begin{tikzpicture}
\draw (0,3) node[anchor=north east] {$\Omega$} rectangle (4,0);
\foreach \x in {1,2,3} {
	\draw (\x,0) -- (\x,3);
} 
\node at (0.5,1.5) {$A_1$};
\node at (1.5,1.5) {$A_2$};
\node at (2.5,1.5) {$\cdots$};
\node at (3.5,1.5) {$A_n$};
\end{tikzpicture}
```

:::


√úblicherweise ist es einfach, eine Aufteilung des Wahrscheinlichkeitsraums zu erhalten, indem man eine Population entsprechend einer kategorischen Variable aufteilt, wie beispielsweise Geschlecht oder Blutgruppe usw.




 Wenn wir eine Aufteilung des Ereignisraums haben, k√∂nnen wir sie verwenden, um die Wahrscheinlichkeiten anderer Ereignisse in demselben Wahrscheinlichkeitsraum zu berechnen.

::: {.callout-warning appearance="default"}
## Definition "Gesamtwahrscheinlichkeit"
Gegeben eine Aufteilung $A_1,\ldots,A_n$  eines Wahrscheinlichkeitsraums $\Omega$, kann die Wahrscheinlichkeit jedes anderen Ereignisses $B$ des selben Wahrscheinlichkeitsraums mit der folgenden Formel berechnet werden:

$$
P(B) = \sum_{i=1}^n P(A_i\cap B) = \sum_{i=1}^n P(A_i)P(B|A_i).
$$

:::



::: {.callout-caution appearance="default"}
## Beweis

Der Beweis dieses Satzes ist recht einfach. Da  $A_1,\ldots,A_n$ eine Aufteilung von $\Omega$ ist, haben wir:

$$
B = B\cap \Omega = B\cap (A_1\cup \cdots \cup A_n) = (B\cap A_1)\cup \cdots \cup (B\cap A_n).
$$

Und alle Ereignisse dieser Vereinigung sind paarweise ausschlie√üend, da $A_1,\ldots,A_n$. 
Daher gilt:

$$
\begin{aligned}
P(B) &= P((B\cap A_1)\cup \cdots \cup (B\cap A_n)) = P(B\cap A_1)+\cdots + P(B\cap A_n) =\\
&= P(A_1)P(B|A_1)+\cdots + P(A_n)P(B|A_n) = \sum_{i=1}^n P(A_i)P(B|A_i).
\end{aligned}
$$



```{r}
#| label: plotgesamtprob
#| fig-width: 6
#| fig-height: 5
# Basis-Plot vorbereiten
plot(NULL, 
     xlim = c(-0.5, 4), 
     ylim = c(0, 3), 
     type = "n", 
     xlab = "", ylab = "", 
     axes = FALSE, 
     asp = 1)

# Rechteck f√ºr Omega
rect(0, 0, 4, 3, border = "black")
text(0, 3, expression(Omega), pos = 2)


# A_i Beschriftungen
text(0.5, 0.2, expression(A[1]))
text(1.5, 0.2, expression(A[2]))
text(2.5, 0.2, expression(cdots))
text(3.5, 0.2, expression(A[n]))

# Funktion zum Zeichnen einer gef√ºllten Ellipse
draw_filled_ellipse <- function(center.x, center.y, a, b, col = "skyblue", border = "black", n = 200) {
  t <- seq(0, 2*pi, length.out = n)
  x <- center.x + a * cos(t)
  y <- center.y + b * sin(t)
  polygon(x, y, col = col, border = border)
}

# Ellipse B (gef√ºllt mit Skyblue)
draw_filled_ellipse(2, 1.5, 1.9, 1, col = "skyblue")

# B-Beschriftung
text(2.5, 2.3, expression(B), cex=1.5)

# Funktion zum F√ºllen der Schnittbereiche A_i ‚à© B
fill_intersection <- function(xmin, xmax, center.x, center.y, a, b, col = rgb(0.4, 0.6, 1, 0.3)) {
  t <- seq(0, 2*pi, length.out = 500)
  x <- center.x + a*cos(t)
  y <- center.y + b*sin(t)
  idx <- which(x >= xmin & x <= xmax)
  polygon(c(x[idx], rev(x[idx])), c(y[idx], rev(y[idx])), col = col, border = NA)
}

# Schnittbereiche f√ºllen
fill_intersection(0, 1, 2, 1.5, 1.9, 1)
fill_intersection(1, 2, 2, 1.5, 1.9, 1)
fill_intersection(2, 3, 2, 1.5, 1.9, 1)
fill_intersection(3, 4, 2, 1.5, 1.9, 1)

# Beschriftungen der Schnittbereiche
text(0.6, 1.5, expression(A[1] %*% B), cex = 0.8)
text(1.5, 1.5, expression(A[2] %*% B), cex = 0.8)
text(2.5, 1.5, expression(cdots), cex = 0.8)
text(3.4, 1.5, expression(A[n] %*% B), cex = 0.8)

# Vertikale Trennlinien
abline(v = 1:3, col = "black")
```

:::




::: {.callout-tip appearance="default"}
## Beispiel: Diagnose

Ein Symptom $S$ kann durch eine Krankheit $K$ verursacht werden, kann aber auch bei Personen ohne die Krankheit auftreten. In einer Population betr√§gt der Anteil der Menschen mit der Krankheit 0,2. Es ist bekannt, dass 90 % der Personen mit der Krankheit das Symptom aufweisen, w√§hrend nur 40 % der Personen ohne die Krankheit es haben.

Wie gro√ü ist die Wahrscheinlichkeit, dass eine zuf√§llig ausgew√§hlte Person aus der Bev√∂lkerung das Symptom hat?
:::

Um diese Frage zu beantworten, k√∂nnen wir den Satz von der vollst√§ndigen Wahrscheinlichkeit unter Verwendung der Aufteilung $\{K,\overline K\}$ anwenden:

$$
P(S) = P(K)P(S|K)+P(\overline K)P(S|\overline K) = 0.2\cdot 0.9 + 0.8\cdot 0.4 = 0.5.
$$
Das hei√üt, die H√§lfte der Bev√∂lkerung hat das Symptom. 

*Tats√§chlich handelt es sich um einen gewichteten Mittelwert der Wahrscheinlichkeiten!*



::: {.callout-tip appearance="default"}
## Beispiel Wahrscheinlichkeitsbaum
Das Ergebnis der vorherigen Frage wird durch den Wahrscheinlichkeitsbaum des Wahrscheinlichkeitsraums noch klarer:

```{r}
#| label: krankheitsbaum
#| engine: tikz
\begin{tikzpicture}[
  grow'=right,
  level 1/.style ={level distance=2cm, sibling distance=1.6cm, parent anchor=east, child anchor=west},
  level 2/.style ={level distance=2cm, sibling distance=0.8cm},
  level 3/.style ={level distance=1.5cm, sibling distance=0.8cm, dashed},
  level 4/.style ={level distance=3cm, sibling distance=0.8cm, dashed},
  prob/.style={font=\footnotesize,above}
]

% Baumstruktur
\node (root) {}
  child {node (krank) {K}
    child {node (symptom_krank) {S}
      child {node[red] (ks) {$\text{K}, \text{S}$}
        child {node[red] (ksval) {$0{,}18$} edge from parent node[prob] {$0{,}2 \cdot 0{,}9$}}
      }
      edge from parent node[prob] {$0{,}9$}
    }
    child {node (kein_symptom_krank) {$\overline S$}
      child {node (kns) {$\text{K}, \overline{\text{S}}$}
        child {node (knsval) {$0{,}02$} edge from parent node[prob] {$0{,}2 \cdot 0{,}1$}}
      }
      edge from parent node[prob,below] {$0{,}1$}
    }
    edge from parent node[prob] {$0{,}2$}
  }
  child {node (nicht_krank) {$\overline K$}
    child {node (symptom_gesund) {S}
      child {node[red] (nks) {$\overline{\text{K}}, \text{S}$}
        child {node[red] (nksval) {$0{,}32$} edge from parent node[prob] {$0{,}8 \cdot 0{,}4$}}
      }
      edge from parent node[prob] {$0{,}4$}
    }
    child {node (kein_symptom_gesund) {$\overline S$}
      child {node (nk_ns) {$\overline{\text{K}}, \overline{\text{S}}$}
        child {node (nk_ns_val) {$0{,}48$} edge from parent node[prob] {$0{,}8 \cdot 0{,}6$}}
      }
      edge from parent node[prob,below] {$0{,}6$}
    }
    edge from parent node[prob,below] {$0{,}8$}
  };

% Beschriftung oberhalb der Ebenen, ohne scope
\node[text width=3.5cm, align=center] at ([yshift=1cm]ksval.north) {Wahrscheinlichkeit};
\node[font=\bfseries, text width=2.5cm, align=center] at ([yshift=1cm]ks.north) {$\Omega$};
\node[text width=2.5cm, align=center] at ([yshift=1cm]symptom_krank.north) {Symptom};
\node[text width=2.5cm, align=center] at ([yshift=1cm]krank.north) {Krankheit};

\end{tikzpicture}

```

$$
\begin{aligned}
P(S) &= P(K\cap S) + P(\overline K\cap S) = P(K)\cdot P(S|K)+P(\overline K)\cdot P(S|\overline K)\\
& = 0.2\cdot 0.9+ 0.8\cdot 0.4 = 0.18 + 0.32 = 0.5.
\end{aligned}
$$
:::









## Satz von Bayes

Eine Teilmenge eines Ereignisraums $A_1,\cdots,A_n$ kann auch als eine Menge von m√∂glichen Hypothesen f√ºr ein Ereignis $B$ interpretiert werden.
In solchen F√§llen kann es hilfreich sein, die posteriori Wahrscheinlichkeit $P(A_i|B)$  jeder Hypothese zu berechnen.

::: {.callout-warning appearance="default"}
## Satz von Bayes
Gegeben eine Partition $A_1,\cdots,A_n$ eines Ereignisraums $\Omega$ und ein weiteres Ereignis $B$ desselben Mengenraums, kann die bedingte Wahrscheinlichkeit jedes Ereignisses $A_i$ $i=1,\ldots,n$ unter $B$ mit der folgenden Formel berechnet werden:

$$
P(A_i|B) = \frac{P(A_i\cap B)}{P(B)} = \frac{P(A_i)P(B|A_i)}{\sum_{i=1}^n P(A_i)P(B|A_i)}.
$$
:::


In dem vorherigen Beispiel sollte die Diagnose f√ºr eine Person mit Symptomen gestellt werden.

In diesem Fall k√∂nnen wir $K$ und $\overline K$ als die beiden m√∂glichen Hypothesen f√ºr das Symptom $S$ interpretieren. Die vorab Wahrscheinlichkeiten daf√ºr sind $P(K)=0.2$  und $P(\overline{K})=0.8$. Das bedeutet, dass wenn wir keine Informationen √ºber das Symptom haben, die Diagnose nicht gestellt werden kann.

Allerdings √§ndert sich diese Unsicherheit bez√ºglich der Hypothese, wenn wir das Symptom beobachten. Dann m√ºssen wir die posteriori Wahrscheinlichkeiten berechnen, um zu diagnostizieren, d.h.

$$
P(K|S) \mbox{ und } P(\overline{K}|S)
$$
Zur Berechnung nutzen wir den Satz von Bayes:

$$
\begin{aligned}
P(D|S) &= \frac{P(D)P(S|D)}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.2\cdot 0.9}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.18}{0.5}=0.36,\\
P(\overline{D}|S) &= \frac{P(\overline{D})P(S|\overline{D})}{P(D)P(S|D)+P(\overline{D})P(S|\overline{D})} = \frac{0.8\cdot 0.4}{0.2\cdot 0.9 + 0.8\cdot 0.4} = \frac{0.32}{0.5}=0.64.
\end{aligned}
$$

Wie wir sehen k√∂nnen, hat sich die Wahrscheinlichkeit, die Krankheit zu haben, erh√∂ht. Trotzdem ist die Wahrscheinlichkeit, sie nicht zu haben, immer noch gr√∂√üer als die Wahrscheinlichkeit, sie zu haben. Aus diesem Grund lautet die Diagnose, dass die Person die Krankheit nicht hat.

In diesem Fall wird gesagt, dass das Symptom $S$ nicht entscheidend ist, um die Krankheit zu diagnostizieren.







## Epidemiologie

Einzelne Zweige der Medizin, die eine intensive Nutzung von Wahrscheinlichkeiten vornehmen, sind Epidemiologie und Pr√§ventionsmedizin.

Die Epidemiologie untersucht die H√§ufigkeit und Ursachen von Krankheiten in Populationen, indem sie Risikofaktoren f√ºr Krankheiten und Ziele zur pr√§ventiven Gesundheitsversorgung identifiziert.

In der Epidemiologie ist man daran interessiert, wie h√§ufig ein medizinisches Ereignis ùê∑ (typischerweise eine Krankheit wie Grippe, ein Risikofaktor wie Rauchen oder ein Schutzfaktor wie eine Impfung) auftritt, das als nominale Variable mit zwei 
Kategorien (das Eintreten oder Nicht-Eintreten des Ereignisses) gemessen wird.

Es gibt verschiedene Messgr√∂√üen zur H√§ufigkeit eines medizinischen Ereignisses. Die wichtigsten sind:

- Pr√§valenz
- Inzidenz
- Relatives Risiko
- Odds Ratio


### Pr√§valenz

::: {.callout-warning appearance="default"}
## Definition "Pr√§valenz"
Die Pr√§valenz eines medizinischen Ereignisses $K$ ist der Anteil einer bestimmten Bev√∂lkerung, der von diesem medizinischen Ereignis betroffen ist.

$$
  \mbox{Pr√§valenz}(K) = \frac{\mbox{Anzahl von } K \text{ betroffener}}{\mbox{Gr√∂√üe der Bev√∂lkerung}}
$$

:::

Die Pr√§valenz wird h√§ufig anhand einer Stichprobe gesch√§tzt, indem man den relativen Anteil der Menschen berechnet, die in der Stichprobe von dem Ereignis betroffen sind. Es ist auch √ºblich, diesen Anteil als Prozentangabe auszudr√ºcken.

::: {.callout-tip appearance="default"}
## Beispiel
Um die Pr√§valenz von Grippe zu sch√§tzen, wurde eine Stichprobe von 1.000 Personen untersucht und 150 davon hatten Grippe. 
Daher betr√§gt die Pr√§valenz der Grippe etwa $150/1000 = 0,15\ $ oder 15%.
:::



### Inzidenz

Die **Inzidenz** misst die Wahrscheinlichkeit des Auftretens eines medizinischen Ereignisses in einer Bev√∂lkerung innerhalb eines bestimmten Zeitraums. Die Inzidenz kann als kumulative Anteilszahl oder als Rate gemessen werden.

::: {.callout-warning appearance="default"}
## Definition "kumulative Inzidenz"
Die kumulative Inzidenz eines medizinischen Ereignisses $K$ ist der Anteil der Menschen, die das Ereignis in einem bestimmten Zeitraum erlitten haben, d.h. die Anzahl der neuen F√§lle mit dem Ereignis in diesem Zeitraum geteilt durch die Gr√∂√üe der Risikobev√∂lkerung.
Anzahl neuer F√§lle mit $K$

$$
  R(K)=\frac{\mbox{Anzahl Neuerkrankungen}}{\mbox{Gr√∂√üe der Risikobev√∂lkerung}}
$$
:::

::: {.callout-tip appearance="default"}
## Beispiel 
Eine Bev√∂lkerung enth√§lt zu Beginn 1.000 Personen ohne Grippe und nach zweij√§hriger Beobachtungszeit hatten 160 davon die Grippe. Die Anteilszahl der Grippe-Inzidenz berechnet sich mit

$$
R(K) = \frac{160}{1.000} = 0,16
$$

Die kumulative Inzidenz liegt also beo 16% f√ºr zwei Jahre.
:::

::: {.callout-warning appearance="default"}
## Definition "Inzidenzrate" (Absolutes Risiko)

Die Inzidenz*rate* oder das absolute Risiko eines medizinischen Ereignisse $K$ ist die Anzahl neuer Erkrankungen geteilt durch die Gr√∂√üe der Risikobev√∂lkerung und die Anzahl der Zeiteinheiten in einem bestimmten Zeitraum.

$$
  R(K)=\frac{\mbox{Anzahl Neuerkrankungen}}{\mbox{Gr√∂√üe der Risikobev√∂lkerung}\times \mbox{Anzahl Zeiteinheiten}}
$$

:::


::: {.callout-tip appearance="default"}
## Beispiel
Eine Bev√∂lkerung enth√§lt zu Beginn 1.000 Personen ohne Grippe und nach zweij√§hriger Beobachtungszeit hatten 160 davon die Grippe. 
Wenn man das Jahr als Zeiteinheit betrachtet, betr√§gt die Inzidenzrate der Grippe

$$
R(K) = \frac{160}{1.000 \cdot 2} =0,08 
$$
Die Inzidenzrate liegt also bei 8% Personen pro Jahr.

:::


### Pr√§vlenz oder Inzidenz

 Die Pr√§valenz zeigt an, wie verbreitet ein medizinisches Ereignis ist und 
ist eher ein Ma√ü f√ºr die Belastung des Ereignisses f√ºr die Gesellschaft ohne Ber√ºcksichtigung der gef√§hrdeten Zeit oder wann Personen m√∂glicherweise einem m√∂glichen Risikofaktor ausgesetzt waren

Die Inzidenz liefert Informationen √ºber das Risiko, von dem Ereignis betroffen zu sein.

Die Pr√§valenz kann in Querschnittsstudien zu einem bestimmten Zeitpunkt gemessen werden, w√§hrend zur Messung der Inzidenz eine L√§ngsschnittstudie erforderlich ist, bei der die Individuen √ºber einen bestimmten Zeitraum beobachtet werden.

Beim Verst√§ndnis der Ereignis-√Ñtiologie ist die Inzidenz n√ºtzlicher als die Pr√§valenz: wenn sich beispielsweise die  Inzidenz einer Krankheit in einer Bev√∂lkerung erh√∂ht, dann gibt es einen Risikofaktor, der dies f√∂rdert.

Wenn die Inzidenz f√ºr die Dauer des Ereignisses ungef√§hr konstant ist, ist die Pr√§valenz ungef√§hr das Produkt aus Ereignis-Inzidenz und durchschnittlicher Ereignisdauer.

$$
\text{Pr√§valenz} = \text{Inzidenz} \cdot \text{Dauer}
$$

### Risiko-Vergleich

Um zu bestimmen, ob ein Faktor oder eine Eigenschaft mit dem medizinischen Ereignis verbunden ist, m√ºssen wir das Risiko des medizinischen Ereignisses in zwei Populationen vergleichen, von denen die eine dem Faktor ausgesetzt ist und die andere nicht. Die 
Gruppe von Menschen, die dem Faktor ausgesetzt sind, wird als *Interventionsgruppe* oder *experimentelle Gruppe* bezeichnet und die Gruppe von Menschen, die nicht ausgesetzt sind, als *Kontrollgruppe*.

In der Regel werden die beobachteten F√§lle f√ºr jede Gruppe in einer 2x2-Tabelle wie der folgenden dargestellt.

$$
\begin{array}{|c|c|c|}
\hline
 & \text{erkrankt}\  K & \text{gesund}\  \overline{K}\\ 
\hline
\text{Interventionsgruppe}\newline (exponiert) & a & b\\ 
\hline 
\text{Kontrollgruppe}\newline (nicht\ exponiert) & c & d\\ 
\hline
\end{array}
$$


### Zurechenbares Risiko

::: {.callout-warning appearance="default"}
## Definition "zurechendbares Risiko" (Risikodifferenz)
Das zurechenbare Risiko (*attributales Risiko* oder *Risikodifferenz*) eines medizinischen Ereignisses $K$ f√ºr Personen, die einem Faktor (Exposition) ausgesetzt sind, ist der Unterschied zwischen den absoluten Risiken der Interventionsgruppe und der 
Kontrollgruppe.

$$
  AR(K)=R_{Intervention}(K)-R_{Kontrolle}(K)=\frac{a}{a+b}-\frac{c}{c+d}.
$$

:::

Das zurechenbare Risiko ist das Risiko eines Ereignisses, das spezifisch auf den Einflussfaktor von Interesse zur√ºckzuf√ºhren ist. Beachten Sie, dass das zurechenbare Risiko *positiv* sein kann, wenn das Risiko der Interventionsgruppe *gr√∂√üer* als das Risiko der Kontrollgruppe ist, und umgekehrt.

::: {.callout-tip appearance="default"}
## Beispiel Impfung

Um die Wirksamkeit eines Impfstoffs gegen Grippe zu bestimmen, wurde zu Beginn des Jahres eine Stichprobe von 1.000 Personen ohne Grippe ausgew√§hlt. Die H√§lfte von ihnen wurde geimpft (Interventionsgruppe) und der Rest erhielt ein Placebo (Kontrollgruppe). Die 
folgende Tabelle fasst die Ergebnisse am Ende des Jahres zusammen.

$$
  \begin{array}{|c|c|c|}
  \hline
  & \text{Grippe}\  K & \text{keine Grippe}\  \overline{K}\\ 
  \hline
  \text{Interventionsgruppe}\newline (geimpft) & 20 & 480\\ 
  \hline 
  \text{Kontrollgruppe}\newline (nicht\ geimpft) & 80 & 420\\ 
  \hline
  \end{array}
$$




F√ºr geimpfte Personen liegt das zurechenbare Risiko an Grippe zu erkranken bei

$$
AR(K) = \frac{20}{20+480}-\frac{80}{80+420} = -0.12.
$$
Das bedeutet, dass das Risiko an Grippe zu erkranken in der Gruppe der Geimpften 12% geringer ist als in der Gruppe der Ungeimpften.
:::




### Relatives Risiko

::: {.callout-warning appearance="default"}
## Definition "relatives Risiko"
Das relative Risiko eines medizinischen Ereignisses $K$ f√ºr Personen, die einer Exposition ausgesetzt sind, ist es der Quotient zwischen den Inzidenzen der Behandlungs- und Kontrollgruppe.

$$
  RR(K)=\frac{\mbox{Risiko der Interventionsgruppe}}{\mbox{Risiko der Kontrollgruppe}}=\frac{R_I(K)}{R_K(K)}=\frac{a/(a+b)}{c/(c+d)}
$$
:::

Das relative Risiko vergleicht das Risiko eines medizinischen Ereignisses zwischen der Behandlungs- und Kontrollgruppe.

-  $RR = 1\ \ \rightarrow$ Es gibt keine Verbindung zwischen dem Ereignis und der Exposition.
-  $RR < 1\ \ \rightarrow$ Die Exposition verringert das Risiko des Ereignisses.
-  $RR > 1\ \ \rightarrow$ Die Exposition erh√∂ht das Risiko des Ereignisses.

Je weiter entfern $RR$ von 1 ist, desto st√§rker die Verbindung.

::: {.callout-tip appearance="default"}
## Beispiel Impfung

Um die Wirksamkeit eines Impfstoffs gegen Grippe zu bestimmen, wurde zu Beginn des Jahres eine Stichprobe von 1.000 Personen ohne  Grippe ausgew√§hlt. Die H√§lfte von ihnen wurde geimpft (Behandlungsgruppe) und der Rest erhielt ein Placebo (Kontrollgruppe). Die 
folgende Tabelle fasst die Ergebnisse am Ende des Jahres zusammen.

$$
\begin{array}{|c|c|c|}
\hline
  & \text{Grippe}\  K & \text{keine Grippe}\  \overline{K}\\ 
\hline
\text{Interventionsgruppe}\newline (geimpft) & 20 & 480\\ 
\hline 
\text{Kontrollgruppe}\newline (nicht\ geimpft) & 80 & 420\\ 
\hline
\end{array}
$$


Das relative Risiko f√ºr geimpfte Personen an Grippe zu erkranken ist:

$$
  RR(K) = \frac{20/(20+480)}{80/(80+420)} = 0,25.
$$

Das bedeutet, dass geimpfte Personen im Vergleich zu ungeimpften nur ein Viertel des Risikos haben die Grippe bekommen. Das bedeutet, der Impfstoff verringerte das Risiko einer Grippe um 75%.
:::






### Odds

::: {.callout-warning appearance="default"}
## efinition "Odds"
Die Odds eines medizinischen Ereignisses $K$ in einer Bev√∂lkerung ist der Quotient zwischen den Personen, die das Ereignis erlitten haben und den Personen, die es nicht in einem bestimmten Zeitraum erlitten haben.

$$
	ODDS(K)=\frac{\mbox{Neue F√§lle mit}\ K}{\mbox{F√§lle ohne}\ K}=\frac{P(K)}{P(\overline K)}
$$

:::

Im Gegensatz zur Inzidenz, die ein Anteil von weniger als oder gleich 1 ist, kann das Odds gr√∂√üer als 1 sein. Es ist jedoch m√∂glich, das Odds in eine Wahrscheinlichkeit umzuwandeln, indem man folgende Formel verwendet:

$$
  P(K) = \frac{ODDS(K)}{ODDS(K)+1}
$$

::: {.callout-tip appearance="default"}
## Beispiel
Eine Bev√∂lkerung enth√§lt initial 1.000 Personen ohne Grippe und nach einem Jahr haben 160 von ihnen die Grippe bekommen. 
Das Odds der Grippe ist somit 
$$
ODDS(\text{Grippe}) = \frac{160}{840} = 0,1905.
$$
:::

Beachten Sie, dass die Inzidenz $160/1000 =0,16 $ betr√§gt.




### Odds Ratio

::: {.callout-warning appearance="default"}
## Definition "Odds Ratio"
Die Odds Ratio eines medizinischen Ereignisses $K$ f√ºr Personen, die einer Exposition ausgesetzt sind, ist der Quotient zwischen dem Odds des Ereignisses in der Behandlungs- und Kontrollgruppe.

$$
  OR(K)=\frac{\mbox{Odds in Interventionsgruppe}}{\mbox{Odds in Kontrollgruppe}}=\frac{a/b}{c/d}=\frac{ad}{bc}
$$

:::

Die Odds Ratio vergleicht die Odds eines medizinischen Ereignisses zwischen der Behandlungs- und Kontrollgruppe. Die Interpretation ist √§hnlich wie beim relativen Risiko.

- $OR=1$ $\Rightarrow$ Es gibt keine Verbindung zwischen dem Ereignis und der Exposition.
- $OR<1$ $\Rightarrow$ Die Exposition verringert das Risiko des Ereignisses.
- $OR>1$ $\Rightarrow$  Die Exposition erh√∂ht das Risiko des Ereignisses.

Je weiter $OR$ weg von 1 ist, desto st√§rker ist die Verbindung.

::: {.callout-tip appearance="default"}
## Beispiel Impfung
Um die Wirksamkeit eines Impfstoffs gegen Grippe zu bestimmen, wurde zu Beginn des Jahres eine Stichprobe von 1.000 Personen ohne Grippe ausgew√§hlt. Die H√§lfte von ihnen wurde geimpft (Interventionsgruppe) und der Rest erhielt ein Placebo (Kontrollgruppe). Die 
folgende Tabelle fasst die Ergebnisse am Ende des Jahres zusammen.

$$
\begin{array}{|c|c|c|}
\hline
 & \text{Grippe}\  K & \text{keine Grippe}\  \overline{K}\\ 
\hline
\text{Interventionsgruppe}\newline (geimpft) & 20 & 480\\ 
\hline 
\text{Kontrollgruppe}\newline (nicht\ geimpft) & 80 & 420\\ 
\hline
\end{array}
$$

Die Odds Ratio der Grippe f√ºr geimpfte Personen ist:

$$
  OR(K) = \frac{20/480}{80/420} = 0,21875.
$$

:::

Das bedeutet, dass die Chancen, an Grippe zu erkranken (im Vergleich dazu, nicht zu erkranken), bei geimpften Personen fast ein F√ºnftel derer bei ungeimpften Personen betragen. Das hei√üt: Auf etwa 22 geimpfte Personen mit Grippe kommen etwa 100 ungeimpfte Personen mit Grippe.




### Relatives Risiko vs Odds ratio
Relatives Risiko und Odds Ratio sind zwei Kennzahlen, aber ihre Interpretation unterscheidet sich leicht. W√§hrend 
das Relative Risiko einen Vergleich der Risiken zwischen den Behandlungs- und Kontrollgruppen ausdr√ºckt, beschreibt die Odds Ratio 
einen Vergleich der *Chancen*, was nicht dasselbe wie das *Risiko* ist. Daher bedeutet ein Odds Ratio von 2 nicht, dass die 
Behandlungsgruppe das doppelte Risiko hat, das medizinische Ereignis zu erwerben.

Die Interpretation des Odds Ratio ist komplizierter, da sie kontrarfaktisch ist und uns angibt, wie viele Male das Ereignis in der Behandlungsgruppe h√§ufiger ist im Vergleich zur Kontrollgruppe unter der Annahme, dass in der Kontrollgruppe das Ereignis so 
h√§ufig wie das Nicht-Ereignis ist.

Der Vorteil des Odds Ratio besteht darin, dass sie nicht von der Pr√§valenz oder der Inzidenz des Ereignisses abh√§ngt und notwendigerweise verwendet werden muss, wenn die Anzahl der Menschen mit dem medizinischen Ereignis in beiden Gruppen willk√ºrlich 
ausgew√§hlt wird, wie z.B. bei Fall-Kontroll-Studien.


::: {.callout-tip appearance="default"}
## Beispiel Rauchen und Lungenkrebs

Um die Assoziation zwischen Lungenkrebs und Rauchen zu bestimmen, wurden zwei Proben ausgew√§hlt (die zweite mit der doppelten Anzahl von Nicht-Krebs-Individuen), wobei folgende Ergebnisse erzielt wurden:

$$
\begin{array}{|c|c|c|}
\hline
 & \text{Krebs} & \text{kein Krebs}\\ 
\hline
\text{Raucher} & 60 & 80\\ 
\hline 
\text{Nichtraucher} & 40 & 320\\ 
\hline
\end{array}
$$

$$
\begin{aligned}
RR(K) &= \frac{60/(60+80)}{40/(40+320)} = 3.86.\\
OR(K) &= \frac{60/80}{40/320} = 6. 
\end{aligned}
$$








$$
\begin{array}{|c|c|c|}
\hline
 & \text{Krebs} & \text{kein Krebs}\\ 
\hline
\text{Raucher} & 60 & 160\\ 
\hline 
\text{Nichtraucher} & 40 & 640\\ 
\hline
\end{array}
$$

$$
\begin{aligned}
RR(K) &= \frac{60/(60+160)}{40/(40+640)} = 4.64.\\
OR(K) &= \frac{60/160}{40/640} = 6. 
\end{aligned}
$$

Das Relative Risiko ver√§ndert sich, wenn wir die Inzidenz oder Pr√§valenz des Ereignisses (Lungenkrebs) √§ndern, w√§hrend sich die Odds Ratio nicht ver√§ndert.

:::



Die Beziehung zwischen dem Relative Risiko und dem Odds Ratio wird durch die folgende Formel gegeben:


$$
  RR = \frac{OR}{1-R_{Kontrolle}+R_{Kontrolle}\cdot OR}=OR \cdot \frac{1-R_{Intervention}}{1-R_{Kontrolle}}
$$
Die Odds Ratio √ºbersch√§tzt stets das Relative Risiko, wenn es gr√∂√üer als 1 ist, und untersch√§tzt es, wenn es kleiner als 1 ist. Allerdings sind Relatives Risiko und Odds Ratio f√ºr seltene medizinische Ereignisse (mit sehr kleiner Pr√§valenz oder Inzidenz) fast gleich.

```{r}
#| label: ORvsRR
#| fig-width: 7
#| fig-height: 5
r <- c(0.01, 0.05, 0.1, 0.2, 0.3, 0.5)
# odds ratio
or <- seq(0,10,0.1)
# empty plot
plot(1, 
     type="n", 
     xlim=c(0,9), 
     ylim=c(0,10), 
     xlab="Relatives Risiko", 
     ylab="Odds Ratio",  
     xaxt="n", 
     yaxt="n")
axis(2, at=0:10, las=2)
axis(1, at=0:9)
abline(h=0:10, v=0:9, col="gray", lty=3)
title("Relatives Risiko vs Odds Ratio", line=3)
# color scale
library(RColorBrewer) 
cols <- brewer.pal(n=9,name="Blues")
# Relative risk computation and plot
for(i in 1:length(r)){
  rr <- or/(1-r[i]+r[i]*or)
  lines(rr,or, col=cols[i+3], )
  mtext(r[i],3, at=tail(rr,1))
}
mtext("Risiko in Kontrollgruppe",3,1)

```









## Diagnosetests

In der Epidemiologie ist es √ºblich, diagnostische Tests zur Diagnose von Krankheiten zu verwenden. Im Allgemeinen sind diese jedoch nicht vollst√§ndig zuverl√§ssig und haben ein gewisses Risiko einer Fehldiagnose, wie in der folgenden Tabelle dargestellt:


$$
\begin{array}{|c|c|c|}
\hline
 & \text{krank}\ K & \text{gesund}\ \overline K\\ \hline
\text{Testergebnis} & \textcolor{green}{\text{richtig positiv}} & \textcolor{red}{\text{falsch positiv}} \\
positiv \mathbf{+} & (RP) & (FP) \\ \hline 
\text{Testergebnis}  & \textcolor{red}{\text{falsch negativ}} & \textcolor{green}{\text{richtig negativ}} \\ 
negativ \mathbf{-} &  (FN) &  (RN)\\\hline
\end{array}
$$

Die Leistung eines diagnostischen Tests h√§ngt von den folgenden beiden Wahrscheinlichkeiten ab:

- Sensitivit√§t und
- Spezifit√§t



### Sensitivit√§t und Spezifit√§t

::: {.callout-warning appearance="default"}
## Definition "Sensitivit√§t"

Die Sensitivit√§t eines diagnostischen Tests ist der Anteil positiver Ergebnisse bei erkrankten Personen.

$$
P(+|K)=\frac{RP}{RP+FN}
$$
:::


::: {.callout-warning appearance="default"}
## Definition "Spezifit√§t"
Die Spezifit√§t eines diagnostischen Tests ist der Anteil negativer Ergebnisse bei gesunden Personen.

$$
P(-|\overline{K})=\frac{RN}{RN+FP}
$$
:::



Ein Test mit hoher 
Sensitivit√§t wird die Krankheit bei den meisten erkrankten Personen erkennen, jedoch auch mehr falsche Positive produzieren als ein weniger empfindlicher Test. Daher ist ein positives Ergebnis eines Tests mit hoher Sensitivit√§t nicht n√ºtzlich zur Best√§tigung der Krankheit, aber ein negatives Ergebnis ist n√ºtzlich zum Ausschluss der Krankheit, da es selten bei erkrankten Personen zu falsch-negativen Ergebnissen kommt.

Andererseits wird ein Test mit hoher Spezifit√§t die Krankheit bei den meisten gesunden Personen ausschlie√üen, jedoch auch mehr falsche Negative produzieren als ein weniger spezifischer Test. Daher ist ein negatives Ergebnis eines Tests mit hoher Spezifit√§t nicht n√ºtzlich zum Ausschluss der Krankheit. Aber ein positives Ergebnis ist n√ºtzlich zur Best√§tigung der Krankheit, da es selten bei gesunden Personen zu falsch-positiven Ergebnissen kommt.

Die Entscheidung f√ºr einen Test mit gr√∂√üerer Sensitivit√§t oder einen Test mit gr√∂√üerer Spezifit√§t h√§ngt von Art der Krankheit und dem Ziel des Tests ab. Im Allgemeinen werden wir einen sensitiven Test verwenden, wenn:

- die Krankheit ernst ist und es wichtig ist, sie zu erkennen.
- die Krankheit behandelbar ist.
- Falsch-positive Ergebnisse keine schweren Sch√§den verursachen.

Und wir werden einen spezifischen Test verwenden, wenn:

- die Krankheit ernst ist, aber schwierig oder unm√∂glich zu behandeln ist.
- falsch-positive Ergebnisse schwerwiegende Sch√§den verursachen.
- die Behandlung von falsch-positiven F√§llen gef√§hrliche Folgen haben kann.



### pr√§diktive Werte

Der wichtigste Aspekt eines diagnostischen Tests ist jedoch seine Vorhersagekraft, die mit den folgenden beiden posterioren Wahrscheinlichkeiten gemessen wird:

- positiv pr√§diktiver Wert und
- negativ pr√§diktiver Wert

::: {.callout-warning appearance="default"}
## Definition "positiv pr√§diktiver Wert
Der positiv pr√§diktive Wert eines diagnostischen Tests ist der Anteil von Personen 
mit der Krankheit unter allen Personen mit einem positiven Ergebnis.

$$
P(K|+) = \frac{RP}{RP+FP}
$$
:::



::: {.callout-warning appearance="default"}
## Definition "Negativ pr√§diktiver Wert"
Der negativ pr√§diktive Wert eines diagnostischen Tests ist der Anteil von Personen 
ohne die Krankheit unter allen Personen mit einem negativen Ergebnis.

$$
P(\overline{K}|-) = \frac{RN}{RN+FN}
$$

:::



Positiv und negativ pr√§diktive Werte erm√∂glichen es, die Krankheit zu best√§tigen oder auszuschlie√üen, besonders,  wenn sie mindestens einen Schwellenwert von 0,5 erreichen.

$PPW>0.5 \ \Rightarrow\ $ Hinweis auf das Vorliegen der Krankheit

$PPV<0.5 \ \Rightarrow\ $ Hinweis auf das Nichtvorliegen der Krankheit

Diese Wahrscheinlichkeiten h√§ngen jedoch von der H√§ufigkeit der Krankheit $P(K)$ ab und k√∂nnen aus der Sensitivit√§t und Spezifit√§t des  diagnostischen Tests unter Verwendung des Satzes von Bayes berechnet werden. 

$$
\begin{aligned}
PPW=P(K|+) &= \frac{P(K)P(+|K)}{P(K)P(+|K)+P(\overline{K})P(+|\overline{K})}\\
NPW=P(\overline{K}|-) &= \frac{P(\overline{K})P(-|\overline{K})}{P(K)P(-|K)+P(\overline{K})P(-|\overline{K})}
\end{aligned}
$$


Daher steigt der positiv pr√§diktive Wert bei h√§ufigen Krankheiten und der negativ pr√§diktive Wert bei seltenen Krankheiten.





::: {.callout-tip appearance="default"}
## Beispiel 
Bei 1.000 Personen wurde ein Grippetest durchgef√ºhrt. Die Ergebnisse sind in der Tabelle dargestellt.


$$
\begin{array}{|c|c|c|}
\hline
 & \text{Grippe}\ K & \text{keine Grippe}\ \overline K\\ \hline
\text{Testergebnis} + & 95 & 90 \\ \hline 
\text{Testergebnis} - & 5 & 810\\\hline
\end{array}
$$



- Die Pr√§valenz der Grippe ist $P(K) = \frac{95+5}{1000} = 0,1$.
- Die Sensitivit√§t des Tests ist $P(+|K) = \frac{95}{95+5}= 0,95$. 
- Die Spezifit√§t des Tests ist $P(-|\overline{K}) = \frac{810}{90+810}=0,9$.
- Der positiv pr√§diktive Wert ist $PPW = P(K|+) = \frac{95}{95+90} = 0,5135$. Da dieser Wert √ºber 0,5 liegt, bedeutet dies, dass wir bei einem positiven Testergebnis wahrscheinlich wirklich Grippe gefunden haben. Allerdings wird das Vertrauen in diesesehr gering, da der Wert sehr nahe an 0,5 liegt.
- Der negativ pr√§diktive Wert ist $NPW = P(\overline{K}|-) = \frac{810}{5+810} = 0,9939$. Da dieser Wert fast 1 ist, bedeutet dies, dass es fast sicher ist, dass eine Person keine Grippe hat, wenn sie ein negatives Testergebnis erh√§lt.

Daher ist dieser Test sehr leistungsf√§hig, um die Grippe auszuschlie√üen, aber nicht so leistungsf√§hig, um sie zu best√§tigen.
:::




### Likelihood Ratio

Die folgenden Ma√üe werden normalerweise aus Sensitivit√§t und Spezifit√§t abgeleitet.

::: {.callout-warning appearance="default"}
## Definition "Positive Likelihood Ratio"
Die positive Likelihood Ratio eines diagnostischen Tests ist das Verh√§ltnis zwischen der Wahrscheinlichkeit positiver Ergebnisse bei Personen mit der Krankheit und gesunden Personen.

$$
LR+=\frac{P(+|K)}{P(+|\overline{K})} = \frac{\mbox{Sensitivit√§t}}{1-\mbox{Spezifit√§t}}
$$
:::


::: {.callout-warning appearance="default"}
## Definition "Negative Likelihood Ratio"
Die negative Likelihood Ratio eines diagnostischen Tests ist das Verh√§ltnis zwischen der Wahrscheinlichkeit negativer Ergebnisse bei Personen mit der Krankheit und gesunden Personen.

$$
LR-=\frac{P(-|K)}{P(-|\overline{K})} = \frac{1-\mbox{Sensitivit√§t}}{\mbox{Spezifit√§t}}
$$
:::


Die positive Likelihood Ratio kann als die Anzahl interpretiert werden, wie oft ein positives Ergebnis bei Personen mit der Krankheit gegen√ºber Personen ohne sie wahrscheinlicher ist.

Andererseits kann die negative Likelihood Ratio als die Anzahl interpretiert werden, wie oft ein negatives Ergebnis bei Personen mit der Krankheit gegen√ºber Personen ohne sie wahrscheinlicher ist.

Post-Test-Wahrscheinlichkeiten k√∂nnen durch Likelihood Ratio aus Pre-Test-Wahrscheinlichkeiten berechnet werden.

$$
P(K|+) = \frac{P(K)P(+|K)}{P(K)P(+|K)+P(\overline{K})P(+|\overline{K})} = \frac{P(K)LR+}{1-P(K)+P(K)LR+}
$$

Daher gilt:

- Eine Likelihood Ratio gr√∂√üer als 1 erh√∂ht die Wahrscheinlichkeit der Erkrankung.
- Eine Likelihood Ratio kleiner als 1 verringert die Wahrscheinlichkeit der Erkrankung.
- Eine Likelihood Ratio von 1 √§ndert die Pre-Test-Wahrscheinlichkeit nicht.


```{r}
#| label: likelihoodRatio
#| fig-width: 8
#| fig-height: 5
pe=seq(0,1, 0.01)
lr=c(0.01, 0.05, 0.1, 0.2, 0.5, 2, 5, 10, 20, 100)
color=hcl(h=seq(0,240,24),
          c=80,l=60)
par(mar=c(3.8, 3.8, 3, 12),
    mgp=c(1.8, 0.4, 0), 
    cex.lab=1, cex.axis=0.8, 
    las=1, 
    tck=-0.02)
plot(pe, pe, type="l", 
     main="Beziehung zwischen Pretest, Posttest\n Wahrscheinlichkeiten und der Likelihood Ratio",
     xlab="Pretest Wahrscheinlichkeit", 
     ylab="Posttest Wahrscheinlichkeit")
for (i in lr) {
  lines(pe,pe*i/(1-pe+pe*i),col=color[which(lr == i)])
}  
abline(h=seq(0, 1, 0.2),
       v=seq(0, 1, 0.2),
       col="gray",
       lty=3)
legend("topright", 
       c("LR-=0.01", "LR-=0.05", "LR-=0.1", "LR-=0.2", "LR-=0.5", "LR+=1",
         "LR+=2", "LR+=5", "LR+=10", "LR+=20", "LR+=100"),
       col=c(color[1:5],"black",
             color[6:10]),
       inset=c(-0.5,0), 
       bty = "n", 
       lty=1, 
       xpd=TRUE,
       title="Likelihood Ratios")

```




